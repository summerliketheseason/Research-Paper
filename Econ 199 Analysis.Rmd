---
title: "Econ 199 Analysis"
author: "Summer Le"
date: '2022-04-06'
output: 
  pdf_document:
    toc: yes
    number_sections: true
    fig_width: 7
    fig_height: 6
    fig_caption: true
    df_print: paged
    highlight: tango
  html_document:
    df_print: paged
    toc: yes
fontsize: 12pt
geometry: margin=1in
header-includes:
- \usepackage{mathtools}
- \usepackage{amssymb}
- \usepackage{pgf}
- \usepackage{tabu}
- \usepackage{pgfplots}
- \usepackage{pgfplotstable}
- \pgfplotsset{compat=1.7}
- \usepackage{tikz}
- \usepackage{yhmath}
- \usepackage{xcolor}
- \usepackage{setspace}
- \doublespacing
fontfamily: mathpazo
chunk_output_type: console
editor_options: 
  markdown: 
    wrap: 72
---

# Library

```{r, message=FALSE, warning=FALSE}
rm(list=ls(all=TRUE))
library(foreign)
library(tis)
library(FNN)
library(tseries)
library(forecast)
library(tidyr) 
library(tinytex)
library(cvTools)
library(boot)
library(rpart)
library(lattice)
library(MASS)
library(car)
require(stats)
require(stats4)
library(KernSmooth)
library(fastICA)
library(cluster)
library(leaps)
library(mgcv)
library(pan)
library(DAAG)
library(TTR)
library(tis)
require("datasets")
require(graphics)
library(xtable)
library(stats)
library(TSA)
library(timeSeries)
library(fUnitRoots)
library(fBasics)
library(timsac)
library(fpp)
library(strucchange)
library(dynlm)
library(lmtest)
library(broom)
library(dplyr)
library(ggplot2)
library(yarrr)
library(seasonal)
library(TimeProjection)
library(moments)
library(quantmod)
library(zoo)
library(RColorBrewer)
library(tseries, quietly = T)
library(forecast, quietly = T)
library(vars)
library(caret)
library("TTR")
library(plotrix)
library(nlstools)
library(fpp2)
library(fma)
library(censReg)
library(tidyverse)
library(POE5Rdata)
library(COUNT)
library(AER)
library(thief)
library(hts)
library(gdata)
library(rugarch)
library(Quandl)
library(geckor)
library(effects)
library(plotly)
library(prophet)
library(glmnet)
library(e1071)
library(class)
library(boot)
library(stringr)
library(vip)
library(pls)
library(processx)
library(rpart)
library(tree)
library(randomForest)
library(rpart.plot)
```

# Macro Data (Zucman)

```{r}
wealth_share = readxl::read_xlsx("/Users/summerletan/Documents/UCLA /Econ 199/PSZ2022AppendixTablesII(Distrib).xlsx", sheet = "Sheet3", col_names = TRUE)

head(wealth_share)

all_wealth_share = readxl::read_xlsx("/Users/summerletan/Documents/UCLA /Econ 199/PSZ2022AppendixTablesII(Distrib).xlsx", sheet = "TE1", col_names = T)
names(all_wealth_share)

all_wealth_share$Year = all_wealth_share$...1

```

### Comparison

#### Wealth distribution

##### 2012-2019

```{r}
fig1 =plot_ly(data=wealth_share, x = ~Year, y = ~wealth_share$`Bottom 50%`,
               type = 'bar', name = 'Bottom 50%') %>%
     add_trace(y = ~wealth_share$`Middle 40%`, name = 'Middle 40%') %>% add_trace(y = ~wealth_share$`Top 10%`, name = 'Top 10%') %>%
    layout(yaxis = list(title = 'Wealth Share'), barmode = 'stack')
fig1


ggplot() + geom_area(aes(x= wealth_share$Year, y = wealth_share$`Top 10%`,
                         fill = "Top 10%"), alpha = 0.8) + 
  geom_area(aes(x= wealth_share$Year, y = wealth_share$`Middle 40%`,fill = "Middle 40%"), 
            alpha = 0.8)+ geom_area(aes(x= wealth_share$Year, 
                                        y = wealth_share$`Bottom 50%`,
                                        fill = "Bottom 50%"), alpha = 0.8) + ggtitle("Wealth Distribution") + xlab("Year") +ylab("Percentage")+scale_fill_brewer(palette="Set3")
```

##### All Periods

#### Composition of Wealth Contribution

Area of improvement

```{r}
wealth_comp = readxl::read_xlsx("/Users/summerletan/Documents/UCLA /Econ 199/PSZ2022AppendixTablesII(Distrib).xlsx", sheet = "Sheet2", col_names = TRUE)
head(wealth_comp)

# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Wealth Distritbution (90-10)(Zucman).pdf",
#     width=8,  height = 8)
ggplot() + geom_area(mapping = aes(x = wealth_comp$Year, 
                                   y = wealth_comp$`Equities (10%)`, 
                                   fill = "Top 10%"), alpha = 0.8) + 
  geom_area(mapping = aes(x = wealth_comp$Year, y = wealth_comp$`Equities (90%)`, 
                          fill = "Bottom 90%"), alpha = 0.8) + 
  ggtitle("Percentage of Equities in Wealth Distribution (Top 10% vs Bottom 90%)") + xlab("Year") + ylab("Percentage")+scale_fill_brewer(palette="Pastel1") 
# dev.off()


```

# Macro Data (WID)

```{r}
wid_data = readxl::read_xlsx("/Users/summerletan/Documents/UCLA /Econ 199/WID_Data_05042022-224339.xlsx", sheet = "Data", col_names = TRUE)

head(wid_data)
tail(wid_data)

#Rename columns for readability

colnames(wid_data) = c("Year", "Top 10%", "Bottom 50%", "Top 1%")

small_wid = wid_data[c(25:34), ]
head(small_wid)
tail(small_wid)

```

# Macro data (Fed)

## Wealth Share

```{r}

fed_data = read.csv("Wealth Quarter.csv", header = T, sep = ',', stringsAsFactors = T)
tail(fed_data)

new_fed = fed_data[,c(1:3)]

new_fed = new_fed %>% spread(Category, Net.worth)

head(new_fed)
tail(new_fed)

fed_ts = ts(new_fed, start = c(1989,3), end = c(2021,4), frequency = 4)
bot50_ts = fed_ts[,"Bottom50"]
head(fed_ts)
next40_ts = fed_ts[,"Next40"]
next9_ts = fed_ts[,"Next9"]
top1_ts = fed_ts[,"Top1"]

plot(bot50_ts, main= "Wealth Distribution", ylim = c(0,45), ylab = "Share")
nberShade()
lines(bot50_ts, col = "forestgreen")
lines(next40_ts, col = "navy")
lines(next9_ts, col = "gold")
lines(top1_ts, col = "purple")
legend("topleft", legend = c("Bottom 50%", "Next 40%", "Next 9%", "Top 1%"),
       text.col = c("forestgreen", "navy","gold", "purple"), cex =.5)


#Add recession indicator to new fed, replace 1 with 100 so when we plot it matches
new_fed$recession = new_recession$indicator



fed_fig = plot_ly(data=new_fed , x = ~Date, y = ~Bottom50,
               name = 'Bottom 50%', type = 'scatter', mode = 'none', stackgroup = 'one', 
               text = ~paste("Bottom 50%","<br> Year - Quarter:", new_fed$Date,
                      "<br> Wealth Share (%):", new_fed$Bottom50),
               hoverinfo = 'text') %>%
    add_trace(y = ~new_fed$Next40, 
              name = 'Next 40%',
              text = ~paste("Next 40%", "<br>Year - Quarter:", new_fed$Date,
                      "<br> Wealth Share (%):", new_fed$Next40),
              hoverinfo = 'text') %>% 
  add_trace(y = ~new_fed$Next9, 
            name = 'Top 9%',
            text = ~paste("Top 9%", "<br>Year - Quarter:", new_fed$Date,
                      "<br> Wealth Share (%):", new_fed$Next9),
        hoverinfo = 'text') %>% 
  add_trace(y = ~new_fed$Top1, 
            name = 'Top 1%', 
            text = ~paste("Top 1%", "<br>Year - Quarter:", new_fed$Date,
                      "<br> Wealth Share (%):", new_fed$Top1),
        hoverinfo = 'text')

# fed_fig = fed_fig %>%  add_bars(y = ~new_fed$recession, 
#             name = 'Recession', marker = list(color = 'rgb(204,204,204)'))
#     
fed_fig = fed_fig %>% layout(yaxis = list(title = 'Wealth Share (%)', titlefont = list(size =30)),
                         xaxis = list(title = 'Date', titlefont = list(size =30)),
                         title = list(text="Wealth Distribution", font = list(size = 30)), legend = list(font = list(size = 30)), margin = list(l=50, r=50, b=100, t=100, pad=4))


fed_fig

# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/wealth_distr.pdf", 
#     width=8,  height = 8)
# fed_fig
# dev.off()
# 
# png("/Users/summerletan/Documents/UCLA /Econ 199/Plots/wealth_distr.png", 
#     width=8,  height = 8)
# fed_fig
# dev.off()



bot50_data = fed_data[fed_data$Category == "Bottom50",]
head(bot50_data)
names(bot50_data)




```
### Wealth gap
```{r}

fed_data = read.csv("Wealth Quarter.csv", header = T, sep = ',', stringsAsFactors = T)
tail(fed_data)


#Calculate top 10
top10_placeholder= fed_data[fed_data$Category== c("Top1", "Next9"),]
head(top10_placeholder)

top10 = aggregate(x=top10_placeholder[,c(3:14)],
                  by = list(top10_placeholder$Date),
                  FUN = sum)
head(top10_placeholder)
head(top10)
top10$Category = rep("Top10", 130)

bottom50 =  fed_data[fed_data$Category== c("Bottom50"),]

head(top10)
head(bottom50)

top10 = top10 %>% relocate("Category", .after = "Group.1")

cat(names(bottom50), sep = ",")

names(top10)[names(top10) == "Group.1"] = "Date"

gap_df = top10-bottom50
head(gap_df)

gap_df$Date = top10$Date

gap_df = gap_df[,-2]

#Tree to see what affects gap
which(names(gap_df) %in% c("Date"))
cat(names(gap_df[,-c(1,2)]), sep = "+")

set.seed(1)
gap_tree = rpart(Net.worth~Assets+Real.estate+Consumer.durables+
                   Corporate.equities.and.mutual.fund.shares+Pension.entitlements
                 +Private.businesses+Other.assets+Liabilities+Home.mortgages+
                   Consumer.credit+Other.liabilities, data =gap_df)
pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Wealth Gap Tree.pdf",
    width=8,  height = 8)
rpart.plot(gap_tree)
dev.off()

set.seed(1)
gap_tree2 = tree(Net.worth~Assets+Real.estate+Consumer.durables+
                   Corporate.equities.and.mutual.fund.shares+Pension.entitlements
                 +Private.businesses+Other.assets+Liabilities+Home.mortgages+
                   Consumer.credit+Other.liabilities, data =gap_df)

plot(gap_tree2)
text(gap_tree2, pretty =1)

```


### Equities share

```{r}
fed_equity= fed_data[,c(1,2,7)]

fed_equity = fed_equity %>% spread(Category, Corporate.equities.and.mutual.fund.shares)

head(fed_equity)

equity_fig = plot_ly(data=fed_equity , x = ~Date, y = ~Bottom50,
               name = 'Bottom 50%', type = 'scatter', mode = 'none', stackgroup = 'one',                text = ~paste("Bottom 50%","<br> Year - Quarter:", fed_equity$Date,
                      "<br> Corporate Equities and Mutual Fund Share (%):", fed_equity$Bottom50),
               hoverinfo = 'text') %>%
    add_trace(y = ~fed_equity$Next40, 
              name = 'Next 40%',
              text = ~paste("Next 40%", "<br>Year - Quarter:", fed_equity$Date,
                      "<br> Corporate Equities and Mutual Fund Share (%):", fed_equity$Next40),
              hoverinfo = 'text') %>% 
  add_trace(y = ~fed_equity$Next9, 
            name = 'Top 9%',
            text = ~paste("Top 9%", "<br>Year - Quarter:", fed_equity$Date,
                      "<br> Corporate Equities and Mutual Fund Share (%):", fed_equity$Next9),
        hoverinfo = 'text') %>% 
  add_trace(y = ~fed_equity$Top1, 
            name = 'Top 1%', 
            text = ~paste("Top 1%", "<br>Year - Quarter:", fed_equity$Date,
                      "<br> Corporate Equities and Mutual Fund Share (%):", fed_equity$Top1),
        hoverinfo = 'text')

# equity_fig = equity_fig %>%  add_bars(y = ~new_fed$recession, 
#             name = 'Recession', marker = list(color = 'rgb(204,204,204)'))
#     
equity_fig = equity_fig %>% layout(yaxis =list(title = 'Corporate Equities and Mutual Fund Share (%)', 
                                          titlefont = list(size =30)),
                             xaxis = list(title = 'Date', titlefont = list(size =30)),
                             title = list(text="Corporate Equities and Mutual Fund Distribution", 
                                      font = list(size =30)),
                             legend = list(font = list(size = 30)),
                             margin = list(l=50, r=50, b=100, t=100, pad=4)) 


equity_fig
```

## Wealth level

```{r}
wealth_lv = read.csv("dfa-networth-levels.csv", header = T, sep = ',', 
                     stringsAsFactors = T)
head(wealth_lv)

#Rename
wealth_lv[names(wealth_lv)== "Corporate.equities.and.mutual.fund.shares"]= "Equities"


lv_fig = plot_ly(data=new_fed , x = ~Date, y = ~wealth_lv[wealth_lv$Category=="Bottom50","Net.worth"],
               name = 'Bottom 50%', type = 'scatter', mode = 'none', stackgroup = 'one', 
               text = ~paste("Bottom 50%","<br> Year - Quarter:", new_fed$Date,
                      "<br> Wealth Share (%):", wealth_lv[wealth_lv$Category=="Bottom50","Net.worth"]),
               hoverinfo = 'text') %>%
    add_trace(y = ~wealth_lv[wealth_lv$Category=="Next40","Net.worth"], 
              name = 'Next 40%',
              text = ~paste("Next 40%", "<br>Year - Quarter:", new_fed$Date,
                      "<br> Wealth Share (%):", wealth_lv[wealth_lv$Category=="Next40","Net.worth"]),
              hoverinfo = 'text') %>% 
  add_trace(y = ~wealth_lv[wealth_lv$Category=="Next9","Net.worth"], 
            name = 'Top 9%',
            text = ~paste("Top 9%", "<br>Year - Quarter:", new_fed$Date,
                      "<br> Wealth Share (%):", wealth_lv[wealth_lv$Category=="Next9","Net.worth"]),
        hoverinfo = 'text') %>% 
  add_trace(y = ~wealth_lv[wealth_lv$Category=="Top1","Net.worth"], 
            name = 'Top 1%', 
            text = ~paste("Top 1%", "<br>Year - Quarter:", new_fed$Date,
                      "<br> Wealth Share (%):", wealth_lv[wealth_lv$Category=="Top1","Net.worth"]),
        hoverinfo = 'text')


lv_fig = lv_fig %>% layout(yaxis = list(title = 'Trillion of U.S. Dollars', 
                                          titlefont = list(size =30)),
                             xaxis = list(title = 'Date', titlefont = list(size =30)),
                             title = list(text="Wealth Level", 
                                      font = list(size =30)),
                            margin = list(l=50, r=50, b=100, t=100, pad=4)) 


lv_fig
```

### Bottom 50%

```{r}

bot50_fig = plot_ly(data=bot50_data , x = ~Date, y = ~Assets,
               name = 'Assets', type = 'scatter', mode = 'none', stackgroup = 'one', 
               text = ~paste("Assets","<br> Year - Quarter:", bot50_data$Date,
                      "<br> Assets(%):", bot50_data$Assets),
               hoverinfo = 'text')
bot50_fig = bot50_fig %>% add_trace(y = ~bot50_data$Real.estate, 
              name = 'Real Estate',
              text = ~paste("Real Estate", "<br>Year - Quarter:", bot50_data$Date,
                      "<br> Real Estate (%):", bot50_data$Real.estate))
bot50_fig = bot50_fig %>% add_trace(y = ~bot50_data$Consumer.durables, 
              name = 'Consumer Durables',
              text = ~paste("Consumer Durables", "<br>Year - Quarter:", bot50_data$Date,
                      "<br> Consumer Durables (%):", bot50_data$Consumer.durables))
bot50_fig = bot50_fig %>% add_trace(y = ~bot50_data$Corporate.equities.and.mutual.fund.shares, 
              name = 'Corporate equities and \n mutual fund shares',
              text = ~paste("Corporate equities and \n 
                            mutual fund shares", "<br>Year - Quarter:", bot50_data$Date,
                      "<br> Corporate equities and \n mutual fund shares (%):", bot50_data$Corporate.equities.and.mutual.fund.shares))

bot50_fig = bot50_fig %>% add_trace(y = ~bot50_data$Pension.entitlements, 
              name = 'Pension Entitlements',
              text = ~paste("Pension Entitlements", "<br>Year - Quarter:", bot50_data$Date,
                      "<br> Pension Entitlements (%):", bot50_data$Pension.entitlements))
bot50_fig = bot50_fig %>% add_trace(y = ~bot50_data$Private.businesses, 
              name = 'Private Businesses',
              text = ~paste("Private Businesses", "<br>Year - Quarter:", bot50_data$Date,
                      "<br> Private Businesses (%):", bot50_data$Private.businesses))
bot50_fig = bot50_fig %>% add_trace(y = ~bot50_data$Other.assets, 
              name = 'Other Assets',
              text = ~paste("Other Assets", "<br>Year - Quarter:", bot50_data$Date,
                      "<br> Other Assets (%):", bot50_data$Other.assets))
bot50_fig = bot50_fig %>% add_trace(y = ~bot50_data$Liabilities, 
              name = 'Liabilities',
              text = ~paste("Liabilities", "<br>Year - Quarter:", bot50_data$Date,
                      "<br> Liabilities (%):", bot50_data$Liabilities))
bot50_fig
```

## Bitcoin ownership distribution

```{r}
btc_own_dist = readxl::read_xlsx("/Users/summerletan/Documents/UCLA /Econ 199/Top 100 Richest Bitcoin Addresses and Bitcoin distribution.xlsx", sheet = "Distribution", col_names = TRUE)
head(btc_own_dist)


# distr_plot = btc_own_dist %>% 
#   ggplot(aes(x=btc_own_dist$`% Addresses (Total)`, 
#              y= btc_own_dist$`% Coins (Total)`, 
#              size = Legend, 
#              color = Colors))+ 
#   geom_point(alpha=0.7) + xlab("% Addresses (Total)") + ylab("% Coins (Total)")+ 
#   theme(legend.title = element_text("legend"), legend.position = "top")
# 
# 
# ggplotly(distr_plot)
# 
# fun_distr_plot =  plot_ly(btc_own_dist, x = ~btc_own_dist$`% Addresses (Total)`, y = ~btc_own_dist$`% Coins (Total)`, type = 'scatter', mode = 'markers',size = ~as.numeric(Legend),
#         marker = list(opacity = 0.5, colors = Colors, sizemod = "diameter")) %>% layout(title = 'Bitcoin Ownership Distribution',
#          xaxis = list(showgrid = FALSE),
#          yaxis = list(showgrid = FALSE))
# fun_distr_plot

#btc_own_dist$Coins


fig2 = plot_ly( data = btc_own_dist, x = ~btc_own_dist$`% Addresses (Total)`, 
        y = ~btc_own_dist$`% Coins (Total)`, 
        type = 'scatter', mode = 'markers',size = ~btc_own_dist$Coins,
        color = ~btc_own_dist$`Balance, BTC`, colors ="Paired",
        marker = list(opacity = 0.5,sizemode = 'diameter'),
        text = ~paste("Number of Coins:", btc_own_dist$Coins,
                      "<br>% Addresses:", btc_own_dist$`% Addresses (Total)`,
                      "<br>% Coins:", btc_own_dist$`% Coins (Total)`),
        hoverinfo = 'text') 

fig2 = fig2 %>% layout(title = list(text='Bitcoin Ownership Distribution',titlefont = list(size =30)),
         xaxis = list(title = '% Addresses', titlefont = list(size =30)),
         yaxis = list(title = '% Coins', titlefont = list(size =30)),
         showlegend=T, 
         legend=list(title=list(text='<b> Number of Coins </b>')),
          margin = list(l=50, r=50, b=100, t=100, pad=4))
fig2

sum(btc_own_dist$`% Addresses (Total)`[1:5])*100 #bottom 90% of bitcoin ownership
sum(btc_own_dist$`% Coins (Total)`[1:5])*100
sum(btc_own_dist$`% Addresses (Total)`[6:12])*100
sum(btc_own_dist$`% Coins (Total)`[6:12])*100 

#Maybe argue that this is just beginning of crypto, comapred to ownership 
#at the beginning for equities? or argue in that direction 

```

15% of addresses hold 0.1-1 coin, which means their ownership ranges
from \$3800 - \$38,000, which is considerably large for investment
portfolio The large orange, yellow and purple pies are probably
institution, in which in our analysis of equities does not include
institutions.

Only a very small portion (almost 0% of address/ only 1 address),
actually owns 100,000 to 1,000,000 coins.

While 25% seemingly only own 0.21% of supply circulating in the market,
those 25% own 0.001-0.01 coin, meaning they own \$40 - \$400 worth of
bitcoin alone, not to mention other coins such as ethererum, terra or
cardano.

# Cryptocurrency

Historical data of top 10 coins (based on market cap)

### Bitcoin

```{r}
getSymbols.yahoo("BTC-USD", periodicity = "daily", env = .GlobalEnv)


btc = as.data.frame(`BTC-USD`)

btc$date = rownames(btc)

dim(btc)

head(btc)
tail(btc)

btc_ts = ts(btc, start =c(year(as.Date("2014-09-17")),
                          as.numeric(format(as.Date("2014-09-17"), "%j"))),
            frequency = 365)

tail(btc_ts)
head(btc_ts)

btc_adj = btc_ts[, "BTC-USD.Adjusted"]

# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Bitcoin.pdf",
#     width=8,  height = 8)
par(mfrow = c(2,1))
plot(btc_adj, xlab = "Time", ylab = "USD", col = "forestgreen",
     main = "Bitcoin Historical Price (Daily)")

plot(btc_ts[,"BTC-USD.Volume"], xlab = "Time", ylab = "Volume", col = "forestgreen",
     main = "Bitcoin Historical Volume (Daily)")

```

#### Bitcoin Return Volatility

For garch, we can even use garch(1,1) with arma(1,0)

```{r}
btc_return = diff(log(btc_adj))
btc_return2 = btc_return^2

# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/tsdisplay of bitcoin return.pdf", 
#     width=8,  height = 8)
tsdisplay(btc_return, main = "Bitcoin Return")
# dev.off()

any(is.na(btc_return))
tail(btc_return)

btc_return = na.omit(btc_return)
# 
# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/tsdisplay of bitcoin volatility.pdf", 
#     width=8,  height = 8)
tsdisplay(btc_return2, main = "Bitcoin Volatility")
dev.off()

btc_sd = sd(btc_adj)

btc_garch11=ugarchspec(variance.model = list(model = "sGARCH", 
                                             garchOrder = c(1, 1)),
                       mean.model = list(armaOrder = c(1, 0), 
                                         include.mean = TRUE),
                       distribution.model = "sstd")
#Check model parameters
btc_garch11@model$pars
#Fit model
btc_garch_mod11 =ugarchfit(btc_garch11,data=btc_return)

# btc_garch_mod11@model$modeldata
#Since this is GARCH model, it would not help if we just plot tsdisplay of resid,
#therefore we will plot acf of squared of standardized residuals

# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Acf of squared of standardized residuals", 
#     width=8,  height = 8)
plot(btc_garch_mod11, which = 11)
dev.off()
# Box test for the residuals 
Box.test(btc_garch_mod11@fit$residuals, type = "Ljung-Box")
#pval =  0.2437 > 0.05 => fail to reject the null => residuals display white noise
# CUSUM plot
# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/CUSUM Plot",
#     width=8,  height = 8)
plot(efp(btc_garch_mod11@fit$residuals~1, type = "Rec-CUSUM"))
dev.off()


btc_vol_fc = ugarchforecast(btc_garch_mod11, data = NULL, n.ahead = 180,n.roll = 0,out.sample = 0)
# btc_vol_fc@forecast$sigmaFor
# btc_vol_fc@model$modeldata$index #why is it 1970..

plot(btc_vol_fc,which = 3)
#Why is actual in August

#Sanity check
plot(btc_return, xlim = c(2022,2023))
plot(btc_adj, xlim = c(2022,2023))
```

#### Volatility (monthly)
```{r}
getSymbols.yahoo("BTC-USD", periodicity = "monthly",  
                 env = .GlobalEnv)
m_btc = `BTC-USD`
m_btc = as.data.frame(m_btc)
m_btc$date = rownames(m_btc)
head(m_btc)
m_btc_ts = ts(m_btc, start = c(2014,10), frequency = 12)
m_btc_adj = m_btc_ts[,"BTC-USD.Adjusted"]
tail(m_btc_adj)
dim(m_btc)

#Monthly return and volatility
m_btc_return = diff(log(m_btc_adj))
tsdisplay(m_btc_return, main = "Monthly return") #Mostly just white noise

m_btc_return2 = m_btc_return^2
tsdisplay(m_btc_return2, main = "Monthly Volatility") #20% volatility, 
#will compare with SP500 later 


#model spec
m_btc_garch=ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1, 1)), mean.model = list(armaOrder = c(2, 1), include.mean = TRUE),
                       distribution.model = "sstd")
#Check model parameters
m_btc_garch@model$pars
#Fit model
m_btc_garch_mod =ugarchfit(m_btc_garch,data=m_btc_return)

m_btc_garch_mod@model$pars

#Since this is GARCH model, it would not help if we just plot tsdisplay of resid,
#therefore we will plot acf of squared of standardized residuals

# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/acf",
#     width=8,  height = 8)
plot(m_btc_garch_mod, which = 11) #spikes are all within barlett window
# dev.off()
Box.test(m_btc_garch_mod@fit$residuals, type = "Ljung-Box")
#pval large -> fail to reject null -> residuals have no corr -> model works
pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/CUSUM Plot",
    width=8,  height = 8)
plot(efp(m_btc_garch_mod@fit$residuals~1, type = "Rec-CUSUM"))
dev.off()
#No structural break -> great

#Forecast:
m_btc_vol_fc = ugarchforecast(m_btc_garch_mod, data = NULL, n.ahead = 30,n.roll = 0,out.sample = 0)


pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Bitcoin Volatility Forecast",
    width=8,  height = 8)
plot(m_btc_vol_fc,which = 3)
dev.off()
#volatility is going to be less + timeline looks right

```

#### SP500 volatility (monthly)
Compare to bitcoin
```{r}
getSymbols.yahoo("^GSPC", periodicity = "monthly",  
                 env = .GlobalEnv,index.class = "Date")
m_sp = as.data.frame(GSPC)
m_sp$date = rownames(m_sp)
head(m_sp)
m_sp_ts = ts(m_sp, start = c(2007,1), frequency = 12)
m_sp_adj = m_sp_ts[,6]
tail(m_sp_adj)
dim(m_sp)

#Monthly return and volatility
m_sp_return = diff(log(m_sp_adj))
tsdisplay(m_sp_return, main = "Monthly return") #Mostly just white noise

m_sp_return2 = m_sp_return^2
tsdisplay(m_sp_return2, main = "Monthly Volatility") #3% then dropped to 2% -> Bitcoin can follow this trend for long term


#model spec
m_sp_garch=ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1, 1)), mean.model = list(armaOrder = c(1, 1), include.mean = TRUE),
                       distribution.model = "sstd")
#Check model parameters
m_sp_garch@model$pars
#Fit model
m_sp_garch_mod =ugarchfit(m_sp_garch,data=m_sp_return)
#Since this is GARCH model, it would not help if we just plot tsdisplay of resid,
#therefore we will plot acf of squared of standardized residuals

plot(m_sp_garch_mod, which = 11) #Almost within barlett window

Box.test(m_sp_garch_mod@fit$residuals, type = "Ljung-Box")
#small pval -> reject null -> need to try again and increase garch order

#---------------------------------------------------

#Second try
m_sp_garch2=ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(2, 2)), mean.model = list(armaOrder = c(1, 1), include.mean = TRUE),
                       distribution.model = "sstd")
#Check model parameters
m_sp_garch2@model$pars
#Fit model
m_sp_garch_mod2 =ugarchfit(m_sp_garch2,data=m_sp_return)
#Since this is GARCH model, it would not help if we just plot tsdisplay of resid,
#therefore we will plot acf of squared of standardized residuals

plot(m_sp_garch_mod2, which = 11) #Almost within barlett window

Box.test(m_sp_garch_mod2@fit$residuals, type = "Ljung-Box")
#Still not white noise

plot(efp(m_sp_garch_mod2@fit$residuals~1, type = "Rec-CUSUM"))
#no structural break which is good
m_sp_garch2=ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(2, 2)), mean.model = list(armaOrder = c(1, 1), include.mean = TRUE),
                       distribution.model = "sstd")
#Check model parameters
m_sp_garch2@model$pars
#Fit model
m_sp_garch_mod2 =ugarchfit(m_sp_garch2,data=m_sp_return)
#Since this is GARCH model, it would not help if we just plot tsdisplay of resid,
#therefore we will plot acf of squared of standardized residuals

plot(m_sp_garch_mod2, which = 11) #Almost within barlett window

Box.test(m_sp_garch_mod2@fit$residuals, type = "Ljung-Box")
#Still not white noise

plot(efp(m_sp_garch_mod2@fit$residuals~1, type = "Rec-CUSUM"))
#---------------------------------------------------

#Third try

m_sp_garch3=ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1, 1)), mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),
                       distribution.model = "sstd")
#Check model parameters
m_sp_garch3@model$pars
#Fit model
m_sp_garch_mod3 =ugarchfit(m_sp_garch3,data=m_sp_return)
#Since this is GARCH model, it would not help if we just plot tsdisplay of resid,
#therefore we will plot acf of squared of standardized residuals

plot(m_sp_garch_mod3, which = 11) #everything is within barlett window

Box.test(m_sp_garch_mod3@fit$residuals, type = "Ljung-Box")
#White noise left

plot(efp(m_sp_garch_mod3@fit$residuals~1, type = "Rec-CUSUM"))
#No structural break


#Forecast:
m_sp_vol_fc = ugarchforecast(m_sp_garch_mod3, data = NULL, n.ahead = 30,n.roll = 0,out.sample = 0)


plot(m_sp_vol_fc,which = 3)
#volatility is going to go down then flatten
```


#### Price Prediction (Prophet)

```{r, message=F, warning=F}

btc_dcmp=decompose(btc_adj, "multiplicative")
# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Bitcoin Dcmp.pdf",
#     width=8,  height = 8)
plot(btc_dcmp)
# dev.off()

head(btc)


#Rename to satisfy ds and y requirement 
new_btc = btc[,c("BTC-USD.Adjusted", "date")]
colnames(new_btc) = c("y", "ds")
tail(new_btc)

#add holidays
suppressMessages(library(dplyr)) 
hld = tis::holidays(c(2014:2022))
us_holidays = data_frame(holiday = "All holidays", 
                         ds =as.Date(as.character(hld), format =  "%Y%m%d"))

#Fit model: daily seasonality + us holidays: smallest rmse
prophet_mod_btc = prophet(new_btc, seasonality.mode = "multiplicative", daily.seasonality = T, weekly.seasonality = F, yearly.seasonality = F, holidays = us_holidays)



##Forecast
future_btc = make_future_dataframe(prophet_mod_btc, periods = 60, freq = "day")
head(future_btc)
tail(future_btc)
btc_price_fc = predict(prophet_mod_btc, future_btc)

plot(prophet_mod_btc, btc_price_fc, xlabel = "Time", ylabel = "Price") +add_changepoints_to_plot(prophet_mod_btc)
#plot(prophet_mod_btc, btc_price_fc)
##All components
prophet_plot_components(prophet_mod_btc, btc_price_fc)
##Just holiday effect
prophet_plot_components(prophet_mod_btc, btc_price_fc)[2]
##Cross Validation
btc_cv = cross_validation(prophet_mod_btc, initial = 730, period = 180, horizon = 365, units = 'days')
btc_cv_perf = performance_metrics(btc_cv)
prop_btc_rmse = btc_cv_perf[,3]
avg_prop_btc_rmse = mean(prop_btc_rmse) #too big

#With all seasonality + us holidays
prophet_mod_btc2 = prophet(new_btc, seasonality.mode = "multiplicative", daily.seasonality = T, weekly.seasonality = T, yearly.seasonality = T, holidays = us_holidays)
##Forecast
future_btc2 = make_future_dataframe(prophet_mod_btc2, periods = 60, freq = "day")
btc_price_fc2 = predict(prophet_mod_btc2, future_btc2)

plot(prophet_mod_btc2, btc_price_fc2) +add_changepoints_to_plot(prophet_mod_btc2)
##All components
prophet_plot_components(prophet_mod_btc2,btc_price_fc2)
##Just holiday effect
prophet_plot_components(prophet_mod_btc2, btc_price_fc2)[2]
##Cross Validation
btc_cv2 = cross_validation(prophet_mod_btc2, initial = 730, period = 180, horizon = 365, units = 'days')
btc_cv_perf2 = performance_metrics(btc_cv2)
prop_btc_rmse2 = btc_cv_perf2[,3]
avg_prop_btc_rmse2 = mean(prop_btc_rmse2)

#only daily seasonality
prophet_mod3 = prophet(new_btc, seasonality.mode = "multiplicative", daily.seasonality = T, weekly.seasonality = F, yearly.seasonality = F)
##Forecast
future3 = make_future_dataframe(prophet_mod3, periods = 60, freq = "day")
btc_price_fc3 = predict(prophet_mod3, future3)

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Bitcoin Fc (avg).pdf",
    width=8,  height = 8)
plot(prophet_mod3, btc_price_fc3, xlabel = "Time", ylabel = "Price") +add_changepoints_to_plot(prophet_mod3)
dev.off()



##All components
prophet_plot_components(prophet_mod3,btc_price_fc3)
##Just holiday effect
prophet_plot_components(prophet_mod3, btc_price_fc3)[2]
##Cross Validation
btc_cv3 = cross_validation(prophet_mod3, initial = 730, period = 180, horizon = 365, units = 'days')
btc_cv_perf3 = performance_metrics(btc_cv3)
prop_btc_rmse3 = btc_cv_perf3[,3]
avg_prop_btc_rmse3 = mean(prop_btc_rmse3)

#Customize yearly seasonality to allow for quicker change
prophet_mod4 = prophet(new_btc, seasonality.mode = "multiplicative", daily.seasonality = T, weekly.seasonality = F, yearly.seasonality = 20)
##Forecast
future4 = make_future_dataframe(prophet_mod4, periods = 60, freq = "day")
btc_price_fc4 = predict(prophet_mod4, future4)

plot(prophet_mod4, btc_price_fc4) +add_changepoints_to_plot(prophet_mod4)
##All components
prophet_plot_components(prophet_mod4,btc_price_fc4)
##Just yearly effect
prophet_plot_components(prophet_mod4, btc_price_fc4)[2]
##Cross Validation
btc_cv4 = cross_validation(prophet_mod4, initial = 730, period = 180, horizon = 365, units = 'days')
btc_cv_perf4 = performance_metrics(btc_cv4)
prop_btc_rmse4 = btc_cv_perf4[,3]
avg_prop_btc_rmse4 = mean(prop_btc_rmse4)

# Fourier for yearly seaosnality
prophet_mod_btc5 = prophet(seasonality.mode = "multiplicative", daily.seasonality = T)
prophet_mod_btc5 = add_seasonality(prophet_mod_btc5, name='yearly', 
                                   fourier.order=10, period =365)
prophet_mod_btc5 =  fit.prophet(prophet_mod_btc5 , new_btc)
##Forecast
future5 = make_future_dataframe(prophet_mod_btc5, periods = 60, freq = "day")
btc_price_fc5 = predict(prophet_mod_btc5, future5)

plot(prophet_mod_btc5, btc_price_fc5) +add_changepoints_to_plot(prophet_mod_btc5)
##All components
prophet_plot_components(prophet_mod_btc5,btc_price_fc5)

##Cross Validation
btc_cv5 = cross_validation(prophet_mod_btc5, initial = 730, period = 180,
                           horizon = 365, units = 'days')
btc_cv_perf5 = performance_metrics(btc_cv5)
prop_btc_rmse5 = btc_cv_perf5[,3]
avg_prop_btc_rmse5 = mean(prop_btc_rmse5)






```

Price correlates with market share + adoption curve -\> impact on
portfolio -\> get to monthly observation

##### Models comparison

```{r}
btc_rmse_df = data.frame("Average RMSE" = c(avg_prop_btc_rmse, avg_prop_btc_rmse2,
                                            avg_prop_btc_rmse3, avg_prop_btc_rmse4,
                                            avg_prop_btc_rmse5),
                         row.names = c("daily seasonality + us holiday", 
                                       " all seasonality + us holidays", 
                                       "only daily seasonality",
                                       "customized yearly seasonality", 
                            "fourier for yearly seasonality"))
print(btc_rmse_df)
#Model with daily seasonality + US holidays has smallest RMSE
head(btc_cv_perf)
head(btc_cv_perf2)
head(btc_cv_perf3)
head(btc_cv_perf4) 
head(btc_cv_perf5) 
#Overall, even if model with customized yearly seasonality has the largest avg 
#rmse, short term prediction seems to have smallest rmse
```

On average, model with daily seasonality (model 3) seems to perform
better, but for short term prediction, model with fourier for yearly
seasonality (model 5) seems to do better -\> we will use both models

#### Best models

```{r}
getSymbols.yahoo("BTC-USD", periodicity = "daily", env = .GlobalEnv)


btc = as.data.frame(`BTC-USD`)

btc$date = rownames(btc)

btc_ts = ts(btc, start =c(year(as.Date("2014-09-17")),
                          as.numeric(format(as.Date("2014-09-17"), "%j"))),
            frequency = 365)

tail(btc_ts)
head(btc_ts)

btc_adj = btc_ts[, "BTC-USD.Adjusted"]

btc_dcmp=decompose(btc_adj, "multiplicative")
# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Bitcoin Dcmp.pdf",
#     width=8,  height = 8)
# plot(btc_dcmp)
# dev.off()

head(btc)


#Rename to satisfy ds and y requirement 
new_btc = btc[,c("BTC-USD.Adjusted", "date")]
colnames(new_btc) = c("y", "ds")
#only daily seasonality
prophet_mod3 = prophet(new_btc, seasonality.mode = "multiplicative", 
                       daily.seasonality = T, weekly.seasonality = F, yearly.seasonality = F)
##Forecast
future3 = make_future_dataframe(prophet_mod3, periods = 60, freq = "day")
btc_price_fc3 = predict(prophet_mod3, future3)

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/BTC Components.pdf",
    width=8,  height = 8, title = "BTC Components",
    )
prophet_plot_components(prophet_mod3, btc_price_fc3)
dev.off()

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Bitcoin Forecast(Daily Seasonality).pdf",
    width=8,  height = 8, title = "Bitcoin Prediction (Daily Seasonality)",
    )
plot(prophet_mod3, btc_price_fc3, xlabel = "Time", ylabel = "Price", title = "Bitcoin Prediction (Daily Seasonality)") +add_changepoints_to_plot(prophet_mod3)

dev.off()

# Fourier for yearly seaosnality
prophet_mod_btc5 = prophet(seasonality.mode = "multiplicative", daily.seasonality = T)
prophet_mod_btc5 = add_seasonality(prophet_mod_btc5, name='yearly', 
                                   fourier.order=10, period =365)
prophet_mod_btc5 =  fit.prophet(prophet_mod_btc5 , new_btc)
##Forecast
future5 = make_future_dataframe(prophet_mod_btc5, periods = 60, freq = "day")
btc_price_fc5 = predict(prophet_mod_btc5, future5)
pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Bitcoin Forecast(Fourier for yearly seaosnality).pdf",
    width=8,  height = 8, title = "Bitcoin Forecast(Fourier for yearly seaosnality)",
    )
plot(prophet_mod_btc5, btc_price_fc5, xlabel = "Time", ylabel = "Price") +add_changepoints_to_plot(prophet_mod_btc5)

dev.off()

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/BTC Components(fourier).pdf",
    width=8,  height = 8, title = "BTC Components(fourier)",
    )
prophet_plot_components(prophet_mod_btc5, btc_price_fc5)
dev.off()

```

### Ethereum

```{r}
eth = getSymbols.yahoo("ETH-USD", periodicity = "daily", env = .GlobalEnv)

eth = as.data.frame(`ETH-USD`)
head(eth)
tail(eth)

eth$date = rownames(eth)

eth_ts = ts(eth, start =c(year(as.Date("2017-11-09")),
                          as.numeric(format(as.Date("2017-11-09"), "%j")))
            ,frequency = 365)
head(eth_ts)

eth_adj = eth_ts[,"ETH-USD.Adjusted"]

plot_ly(x=~eth$date, y=~eth$`ETH-USD.Adjusted`, type = "scatter", mode = "lines",
        line = list(color = "forestgreen"),
        name = "ETH Price") %>% layout("ETH Historical Price", showlegend = T,
                                       xaxis = list(title="Date"),
                                       yaxis = list(title="USD"), 
                                       title = "Ethereum Historical Price")

# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Ethereum.pdf", 
#     width=8,  height = 8)
par(mfrow = c(2,1))
plot(eth_adj, xlab = "Time", ylab = "USD", col = "forestgreen",
     main = "Ethereum Historical Price (Daily)")

plot(eth_ts[,"ETH-USD.Volume"], xlab = "Time", ylab = "Volume", col = "forestgreen",
     main = "Ethereum Historical Volume (Daily)")

```

#### Price Prediction

```{r}
eth_dcmp=decompose(eth_adj, "multiplicative")
# # pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Ethereum Dcmp.pdf", 
#     width=8,  height = 8)
plot(eth_dcmp)
# dev.off()


#Rename to satisfy ds and y requirement 
new_eth = eth[,c("ETH-USD.Adjusted", "date")]
colnames(new_eth) = c("y", "ds")
tail(new_eth)

#add holidays
suppressMessages(library(dplyr)) 
hld = tis::holidays(c(2014:2022))
us_holidays = data_frame(holiday = "All holidays", 
                         ds =as.Date(as.character(hld), format =  "%Y%m%d"))

#Fit model: only daily seasonality
prophet_mod_eth = prophet(new_eth, daily.seasonality = T, seasonality.mode = "multiplicative")


##Forecast
future_eth = make_future_dataframe(prophet_mod_eth, periods = 60, freq = "day")
head(future_eth)
tail(future_eth)
eth_price_fc = predict(prophet_mod_eth, future_eth)
plot(prophet_mod_eth, eth_price_fc) +add_changepoints_to_plot(prophet_mod_eth)

##All components
prophet_plot_components(prophet_mod_eth, eth_price_fc) 
#Peak on wednesday, dip on friday 
#Usually peak around May if yearly
#Daily: peak during 22:00pm - 2am

##Cross Validation
df_cv_eth = cross_validation(prophet_mod_eth, initial = 380, period = 100, horizon = 200, units = 'days') #11 fc
cv_eth_perf = performance_metrics(df_cv_eth)
prop_rmse_eth = head(cv_eth_perf)[3] #RMSE is bad
avg_prop_rmse_eth = mean(prop_rmse_eth$rmse) #off by $307.0044: this is pretty large

#Try with all seasonalities --> lowest RMSE
prophet_mod_eth2 = prophet(new_eth, seasonality.mode = "multiplicative", daily.seasonality = T, weekly.seasonality = T, yearly.seasonality = T)
##Forecast

future_eth2 = make_future_dataframe(prophet_mod_eth2, periods = 60, freq = "day")
eth_price_fc2 = predict(prophet_mod_eth2, future_eth2)

plot(prophet_mod_eth2, eth_price_fc2, xlim = c(2022,2022.6)) +add_changepoints_to_plot(prophet_mod_eth2)
eth_price_fc2[c(1625:1685 ),]

##All components
prophet_plot_components(prophet_mod_eth2, eth_price_fc2) 
#Peak on wednesday, dip on friday 
##Cross Validation
df_cv_eth2 = cross_validation(prophet_mod_eth2, initial = 380, period = 100, horizon = 200, units = 'days') # 11 fc
cv_eth_perf2 = performance_metrics(df_cv_eth2)
prop_rmse_eth2 = head(cv_eth_perf2)[3]
head(cv_eth_perf2)

avg_prop_rmse_eth2 = mean(prop_rmse_eth2$rmse) #307.0044: same

#Add holidays
prophet_mod_eth3 = prophet(new_eth, seasonality.mode = "multiplicative", daily.seasonality = T, weekly.seasonality = T, yearly.seasonality = T, holidays = us_holidays)
##Forecast
future_eth3 = make_future_dataframe(prophet_mod_eth3, periods = 60, freq = "day")
eth_price_fc3 = predict(prophet_mod_eth3, future_eth3)

plot(prophet_mod_eth3, eth_price_fc3) +add_changepoints_to_plot(prophet_mod_eth3)
##All components
prophet_plot_components(prophet_mod_eth3, eth_price_fc3) 
#Peak on wednesday, dip on friday & monday
##Cross Validation
df_cv_eth3 = cross_validation(prophet_mod_eth3,initial = 380, period = 100, horizon = 200, units = 'days') #11 fc
cv_eth_perf3 = performance_metrics(df_cv_eth3)
prop_rmse_eth3 = head(cv_eth_perf3)[3]  #RMSE is worse 
avg_prop_rmse_eth3=mean(prop_rmse_eth3$rmse)

#Adjust trend to make it more flexible, but increase uncertainty
#Fit model: only daily seasonality
prophet_mod_eth4 = prophet(new_eth, daily.seasonality = T, seasonality.mode = "multiplicative",changepoint.prior.scale = 0.3)

##Forecast
future_eth4 = make_future_dataframe(prophet_mod_eth4, periods = 60, freq = "day")
eth_price_fc4 = predict(prophet_mod_eth4, future_eth4)

plot(prophet_mod_eth4, eth_price_fc4) +add_changepoints_to_plot(prophet_mod_eth4)

##All components
prophet_plot_components(prophet_mod_eth4, eth_price_fc4) 
#Peak on wednesday, dip on friday 
#Usually peak around May if yearly
#Daily: peak during 22:00pm - 2am

##Cross Validation
df_cv_eth4 = cross_validation(prophet_mod_eth4, initial = 380, period = 100, horizon = 200, units = 'days') #11 fc
cv_eth_perf4 = performance_metrics(df_cv_eth4)
prop_rmse_eth4 = head(cv_eth_perf4)[3] #RMSE is bad
avg_prop_rmse_eth4 = mean(prop_rmse_eth4$rmse) #off by $375.1077 -> not better

# Default model

prophet_mod_eth6 = prophet(new_eth)


##Forecast
future_eth6 = make_future_dataframe(prophet_mod_eth6, periods = 60, freq = "day")
eth_price_fc6 = predict(prophet_mod_eth6, future_eth6)
plot(prophet_mod_eth6, eth_price_fc6) +add_changepoints_to_plot(prophet_mod_eth6)

##All components
prophet_plot_components(prophet_mod_eth6, eth_price_fc6) 
#Peak on wednesday, dip on friday 
#Usually peak around May if yearly
#Daily: peak during 22:00pm - 2am

##Cross Validation
df_cv_eth6 = cross_validation(prophet_mod_eth6, initial = 380, period = 100, 
                              horizon = 200, units = 'days') #11 fc
cv_eth_perf6 = performance_metrics(df_cv_eth6)
prop_rmse_eth6 = head(cv_eth_perf6)[3] #RMSE is bad
avg_prop_rmse_eth6 = mean(prop_rmse_eth6$rmse) #off by $307.0044: this is pretty large

```

##### Best model

```{r}
#Rename to satisfy ds and y requirement 
new_eth = eth[,c("ETH-USD.Adjusted", "date")]
colnames(new_eth) = c("y", "ds")
tail(new_eth)

# Customize yearly seasonality to allow for quicker change within one year:
prophet_mod_eth5 = prophet(new_eth, daily.seasonality = T, seasonality.mode = "multiplicative",yearly.seasonality = 20)

##Forecast
future_eth5 = make_future_dataframe(prophet_mod_eth5, periods = 60, freq = "day")
eth_price_fc5 = predict(prophet_mod_eth5, future_eth5)

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Ethereum Forecast.pdf",
    width=8,  height = 8)
plot(prophet_mod_eth5, eth_price_fc5, xlabel = "Time", ylabel = "Price") +add_changepoints_to_plot(prophet_mod_eth5)
dev.off()
##All components
pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Ethereum Components.pdf", 
    width=8,  height = 8)
prophet_plot_components(prophet_mod_eth5, eth_price_fc5) 
dev.off()
#Peak on wednesday, dip on friday 
#Usually peak around May if yearly
#Daily: peak during 22:00pm - 2am

##Cross Validation
df_cv_eth5 = cross_validation(prophet_mod_eth5, initial = 380, period = 100, horizon = 200, units = 'days') #11 fc
cv_eth_perf5 = performance_metrics(df_cv_eth5)
prop_rmse_eth5 = head(cv_eth_perf5)[3] 
avg_prop_rmse_eth5 = mean(prop_rmse_eth5$rmse) #Smallest one: $237.4149
```

##### 2020 only
```{r}
getSymbols.yahoo("ETH-USD", periodicity = "daily", env = .GlobalEnv,
                           from = "2020-01-01")

eth2020 = as.data.frame(`ETH-USD`)

eth2020$date = rownames(eth2020)

eth_ts2 = ts(eth2020, start =c(year(as.Date("2020-01-01")),
                          as.numeric(format(as.Date("2020-01-01"), "%j")))
            ,frequency = 365)
head(eth_ts2)

eth_adj2 = eth_ts2[,"ETH-USD.Adjusted"]

new_eth2 = eth2020[,c("ETH-USD.Adjusted", "date")]
colnames(new_eth2) = c("y", "ds")

# model
prophet_mod_eth6 = prophet(new_eth2)
##Forecast
future_eth6 = make_future_dataframe(prophet_mod_eth6, periods = 60, freq = "day")
eth_price_fc6 = predict(prophet_mod_eth6, future_eth6)
plot(prophet_mod_eth6, eth_price_fc6) +add_changepoints_to_plot(prophet_mod_eth6)

##All components
prophet_plot_components(prophet_mod_eth6, eth_price_fc6) 
#Peak on wednesday, dip on friday 
#Usually peak around May if yearly
#Daily: peak during 22:00pm - 2am

##Cross Validation
df_cv_eth6 = cross_validation(prophet_mod_eth6, initial = 366, period = 40, 
                              horizon = 60, units = 'days') #11 fc
cv_eth_perf6 = performance_metrics(df_cv_eth6)
prop_rmse_eth6 = head(cv_eth_perf6)[3] #RMSE is bad
avg_prop_rmse_eth6 = mean(prop_rmse_eth6$rmse)

```


##### Models comparison

```{r}
eth_rmse_df = data.frame("Avg RMSE" = c(avg_prop_rmse_eth, avg_prop_rmse_eth2, avg_prop_rmse_eth3, avg_prop_rmse_eth4, avg_prop_rmse_eth5), 
                         row.names = c("only daily seasonality", 
                                       "all seasonalities",
                                       "all seasonalities + holidays", 
                                       "Adjusted Trend with daily seasonalities",
                                       "Customize yearly seasonality"))
print(eth_rmse_df)

head(cv_eth_perf)
head(cv_eth_perf2)
head(cv_eth_perf3)
head(cv_eth_perf4)
head(cv_eth_perf5)


```

### Tether

```{r}
usdt = getSymbols.yahoo("USDT-USD", periodicity = "daily", env = .GlobalEnv)

usdt = as.data.frame(`USDT-USD`)
head(usdt)
tail(usdt)

usdt$date = rownames(usdt)

usdt_ts = ts(usdt, start =c(year(as.Date("2017-11-09")),
                          as.numeric(format(as.Date("2017-11-09"), "%j"))), 
             frequency = 365)
head(usdt_ts)

usdt_adj = usdt_ts[,"USDT-USD.Adjusted"]

plot_ly(x=~usdt$date, y=~usdt$`USDT-USD.Adjusted`, type = "scatter", mode = "lines",
        line = list(color = "forestgreen"),
        name = "Tether Price") %>% layout("Tether Historical Price", showlegend = T,
                                       xaxis = list(title="Date"),
                                       yaxis = list(title="USD"))
```

#### Price Prediction

```{r}
usdt_dcmp=decompose(usdt_adj, "multiplicative")

plot(usdt_dcmp)

#Rename to satisfy ds and y requirement 
new_usdt = usdt[,c("USDT-USD.Adjusted", "date")]
colnames(new_usdt) = c("y", "ds")
tail(new_usdt)

#add holidays
suppressMessages(library(dplyr)) 
hld = tis::holidays(c(2017:2022))
us_holidays = data_frame(holiday = "All holidays", 
                         ds =as.Date(as.character(hld), format =  "%Y%m%d"))

#Fit model: only daily seasonality
prophet_mod_usdt = prophet(new_usdt)


##Forecast
future_usdt = make_future_dataframe(prophet_mod_usdt, periods = 60, freq = "day")
head(future_usdt)
tail(future_usdt)
usdt_price_fc = predict(prophet_mod_usdt, future_usdt)

# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/USDT Forecast.pdf",
#     width=8,  height = 8)
plot(prophet_mod_usdt, usdt_price_fc) +add_changepoints_to_plot(prophet_mod_usdt)
dev.off()
##All components
# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/USDT Components.pdf",
#     width=8,  height = 8)
prophet_plot_components(prophet_mod_usdt, usdt_price_fc)
dev.off()
#Monday and wednesday dip, thursday peak
##Cross Validation
df_cv_usdt = cross_validation(prophet_mod_usdt, initial = 400, period = 90, horizon = 365,
                               units = 'days')
cv_usdt_perf = performance_metrics(df_cv_usdt)
prop_rmse_usdt = cv_usdt_perf[,3] #not bad at all
avg_rmse_usdt = mean(prop_rmse_usdt)
#Note that price doens't change much -> explains why rmse is so low
head(cv_usdt_perf)

#Try with more seasonality 
prophet_mod_usdt2 = prophet(new_usdt, seasonality.mode = "multiplicative", daily.seasonality = T, weekly.seasonality = T, yearly.seasonality = T)
##Forecast
future_usdt2 = make_future_dataframe(prophet_mod_usdt2, periods = 60, freq = "day")
usdt_price_fc2 = predict(prophet_mod_usdt2, future_usdt2)

plot(prophet_mod_usdt2, usdt_price_fc2) +add_changepoints_to_plot(prophet_mod_usdt2)
##All components
prophet_plot_components(prophet_mod_usdt2, usdt_price_fc2) 

##Cross Validation
df_cv_usdt2 = cross_validation(prophet_mod_usdt2, initial = 400, period = 90, horizon = 365, units = 'days')
cv_usdt_perf2 = performance_metrics(df_cv_usdt2)
prop_rmse_usdt2 = cv_usdt_perf2[,3]  
avg_rmse_usdt2 = mean(prop_rmse_usdt2)
head(cv_usdt_perf2) #RMSEs are higher than with only daily seasonality -> not better

#Add holidays + only daily seasonality
prophet_mod_usdt3 = prophet(new_usdt, seasonality.mode = "multiplicative", daily.seasonality = T, weekly.seasonality = F, yearly.seasonality = F, holidays = us_holidays)
##Forecast
future_usdt3 = make_future_dataframe(prophet_mod_usdt3, periods = 60, freq = "day")
usdt_price_fc3 = predict(prophet_mod_usdt3, future_usdt3)

plot(prophet_mod_usdt3, usdt_price_fc3) +add_changepoints_to_plot(prophet_mod_usdt3)
##All components
prophet_plot_components(prophet_mod_usdt3, usdt_price_fc3) 

##Cross Validation
df_cv_usdt3 = cross_validation(prophet_mod_usdt3, initial = 400, period = 90, horizon = 365, units = 'days')
cv_usdt_perf3 = performance_metrics(df_cv_usdt3)
prop_rmse_usdt3 = cv_usdt_perf3[,3]  #RMSE is not significantly better than when it's without us holidays
avg_rmse_usdt3 = mean(prop_rmse_usdt3)
head(cv_usdt_perf3)
usdt_rmse_df = data.frame("Avg RMSE" = c(prop_rmse_usdt, prop_rmse_usdt2, prop_rmse_usdt3), row.names = c("only daily seasonality", 
                                       "all seasonalities",
                                       "only daily seasonality + holidays"))
print(usdt_rmse_df)
```

##### Models comparison

```{r}
usdt_rmse_df = data.frame("Avg RMSE" = c(mean(prop_rmse_usdt),
                                         mean(prop_rmse_usdt2), 
                                         mean(prop_rmse_usdt3)), 
                          row.names = c("only daily seasonality",
                                        "all seasonalities",
                                        "only daily seasonality + holidays"))
print(usdt_rmse_df)
#Only daily seasonality is the best

```

## SOL

```{r}
getSymbols.yahoo("SOL-USD", periodicity = "daily", env = .GlobalEnv)

sol = as.data.frame(`SOL-USD`)
head(sol)
tail(sol)

sol$date = rownames(sol)

sol_ts = ts(sol, start =c(year(as.Date("2020-04-10")),
                          as.numeric(format(as.Date("2020-04-10"), "%j")))
            , frequency = 365)
head(sol_ts)

sol_adj = sol_ts[,"SOL-USD.Adjusted"]

# plot(sol_adj)

# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Solana.pdf", 
#     width=8,  height = 8)
par(mfrow = c(2,1))
plot(sol_adj, xlab = "Time", ylab = "USD", col = "forestgreen",
     main = "Solana Historical Price (Daily)")

plot(sol_ts[,"SOL-USD.Volume"], xlab = "Time", ylab = "Volume", col = "forestgreen",
     main = "Solana Historical Volume (Daily)")
dev.off()

plot_ly(x=~sol$date, y=~sol$`SOL-USD.Adjusted`, type = "scatter", mode = "lines",
        line = list(color = "forestgreen"),
        name = "Solana Price") %>% layout("Solana Historical Price", showlegend = T,
                                       xaxis = list(title="Date"),
                                       yaxis = list(title="USD"))
```

#### Price Prediction

```{r}
#Rename to satisfy ds and y requirement 
new_sol = sol[,c("SOL-USD.Adjusted", "date")]
colnames(new_sol) = c("y", "ds")
tail(new_sol)
head(new_sol)

#add holidays
suppressMessages(library(dplyr)) 
hld = tis::holidays(c(2020:2022))
us_holidays = data_frame(holiday = "All holidays", 
                         ds =as.Date(as.character(hld), format =  "%Y%m%d"))
```

##### Only daily seasonality

```{r}
#Fit model: only daily seasonality
prophet_mod_sol = prophet(new_sol, daily.seasonality = T, seasonality.mode = "multiplicative")


##Forecast
future_sol = make_future_dataframe(prophet_mod_sol, periods = 30, freq = "day")
head(future_sol)
tail(future_sol)
sol_price_fc = predict(prophet_mod_sol, future_sol)
#sol_price_fc[c(741:802),]
plot(prophet_mod_sol, sol_price_fc) +add_changepoints_to_plot(prophet_mod_sol)

##All components
prophet_plot_components(prophet_mod_sol, sol_price_fc) 
#Monday, Tuesday and Friday dip, peak on wednesday, Sat and Sun
df_cv_sol = cross_validation(prophet_mod_sol, initial = 366, period = 30, horizon = 60,units = 'days') #11 fc
cv_sol_perf = performance_metrics(df_cv_sol)
prop_rmse_sol = cv_sol_perf[,3] #RMSE is really big

head(cv_sol_perf)
avg_prop_rmse_sol = mean(prop_rmse_sol) #Avg rmse = 121, really bad

```

##### daily + weekly + monthly seasonality

```{r}
#Fit model: daily + weekly + yearly seasonality
prophet_mod_sol2 = prophet(new_sol, daily.seasonality = T, weekly.seasonality = T, yearly.seasonality = T , seasonality.mode = "multiplicative")


##Forecast
future_sol2 = make_future_dataframe(prophet_mod_sol2, periods = 60, freq = "day")
head(future_sol2)
tail(future_sol2)
sol_price_fc2 = predict(prophet_mod_sol2, future_sol2)
#sol_price_fc2[c(741:802),]
plot(prophet_mod_sol2, sol_price_fc2) +add_changepoints_to_plot(prophet_mod_sol2)

##All components
prophet_plot_components(prophet_mod_sol2, sol_price_fc2) 
#Tuesday and Friday dip, peak on wednesday, sat and sun
df_cv_sol2 = cross_validation(prophet_mod_sol2, initial = 366, period = 30, horizon = 60,
                               units = 'days')
cv_sol_perf2 = performance_metrics(df_cv_sol2)
prop_rmse_sol2 = cv_sol_perf2[,3]

head(cv_sol_perf2) #RMSE is really big

avg_prop_rmse_sol2 = mean(prop_rmse_sol2) #didn't change compared to above
```

##### daily + weekly + monthly seasonality w/o multiplicative

```{r}

#Fit model: daily + weekly + monthly seasonality w/o multiplicative
prophet_mod_sol3 = prophet(new_sol, daily.seasonality = T, weekly.seasonality = T, yearly.seasonality = T)


##Forecast
future_sol3 = make_future_dataframe(prophet_mod_sol3, periods = 60, freq = "day")
head(future_sol3)
tail(future_sol3)
sol_price_fc3 = predict(prophet_mod_sol3, future_sol3)
plot(prophet_mod_sol3, sol_price_fc3) +add_changepoints_to_plot(prophet_mod_sol3)

##All components
prophet_plot_components(prophet_mod_sol3, sol_price_fc3) 
#Tuesday and Friday dip, peak on wednesday

#Cross validation: different numbers of periods -> significantly different RMSE
#Attempt 1
df_cv_sol31 = cross_validation(prophet_mod_sol3, initial = 366, period = 30, horizon = 60,
                               units = 'days')
cv_sol_perf31 = performance_metrics(df_cv_sol31)
prop_rmse_sol31 = cv_sol_perf31[,3]

head(cv_sol_perf31) #RMSE is really big

mean(prop_rmse_sol31) #$66: smaller than before but still huge

```

##### Default model

```{r}
prophet_mod_sol4 = prophet(new_sol)


##Forecast
future_sol4 = make_future_dataframe(prophet_mod_sol4, periods = 60, freq = "day")
head(future_sol4)
tail(future_sol4)
sol_price_fc4 = predict(prophet_mod_sol4, future_sol4)
plot(prophet_mod_sol4, sol_price_fc4) +add_changepoints_to_plot(prophet_mod_sol4)

##All components
prophet_plot_components(prophet_mod_sol4, sol_price_fc4) 
#Tuesday and Friday dip, peak on wednesday

#Cross validation: 
df_cv_sol4 = cross_validation(prophet_mod_sol4, initial = 366, period = 30, horizon = 60,
                               units = 'days')
cv_sol_perf4 = performance_metrics(df_cv_sol4)
prop_rmse_sol4 = cv_sol_perf4[,3]
mean(prop_rmse_sol4) #66.08935: still huge

head(cv_sol_perf4) #not better
```

##### Fourier (yearly)

```{r}
prophet_mod_sol5 = prophet(seasonality.mode = "multiplicative", daily.seasonality = T)
prophet_mod_sol5 = add_seasonality(prophet_mod_sol5, name='yearly', 
                                   fourier.order=10, period =365)
prophet_mod_sol5 =  fit.prophet(prophet_mod_sol5 , new_sol)

##Forecast
future_sol5 = make_future_dataframe(prophet_mod_sol5, periods = 60, freq = "day")
head(future_sol5)
tail(future_sol5)
sol_price_fc5 = predict(prophet_mod_sol5, future_sol5)
plot(prophet_mod_sol5, sol_price_fc5) +add_changepoints_to_plot(prophet_mod_sol5)

##All components
prophet_plot_components(prophet_mod_sol5, sol_price_fc5) 
#Monday, tuesday, friday dip, peak on wednesday, sat and sun

#Cross validation: 
df_cv_sol5 = cross_validation(prophet_mod_sol5, initial = 400, period = 30,
                              horizon = 60,
                               units = 'days') #10 fc
cv_sol_perf5 = performance_metrics(df_cv_sol5)
prop_rmse_sol5 = cv_sol_perf5[,3]

head(cv_sol_perf5) #RMSEs are big

mean(prop_rmse_sol5) #132, worst
```

##### NNAR

```{r}
nnar_sol = nnetar(sol_adj)
nnar_sol_fc = forecast(nnar_sol, h = 30)
plot(nnar_sol_fc)

train_sol = window(sol_adj, end = c(2021,8))
test_sol = window(sol_adj, start = c(2021,9))
nnar_sol_train = nnetar(train_sol)
nnar_sol_test = forecast(nnar_sol_train, h = 60)
accuracy(nnar_sol_test, test_sol) 

#Neural net is worse than prophet  (so annoying :( )

nnar_fc_func = function(x,h){
  nnar_sol = nnetar(x)
  nnar_sol_fc = forecast(nnar_sol, h = h)
}

nnar_cv = tsCV(sol_adj, nnar_fc_func, h=60)
sqrt(mean(nnar_cv^2, na.rm = T)) #Worse than prophet

```

##### yearly seasonality = 20

```{r}
prophet_mod_sol6 = prophet(new_sol, daily.seasonality = T,  yearly.seasonality = 20 , seasonality.mode = "multiplicative")

##Forecast
future_sol6 = make_future_dataframe(prophet_mod_sol6, periods = 60, freq = "day")
head(future_sol6)
tail(future_sol6)
sol_price_fc6 = predict(prophet_mod_sol6, future_sol6)
#sol_price_fc2[c(741:802),]
plot(prophet_mod_sol6, sol_price_fc6) +add_changepoints_to_plot(prophet_mod_sol6)

##All components
prophet_plot_components(prophet_mod_sol6, sol_price_fc6) 
#Tuesday and Friday dip, peak on wednesday, sat and sun
df_cv_sol6 = cross_validation(prophet_mod_sol6, initial = 366, period = 30, horizon = 60,
                               units = 'days')
cv_sol_perf6 = performance_metrics(df_cv_sol6)
prop_rmse_sol6 = cv_sol_perf6[,3]

head(cv_sol_perf6) #RMSE is really big

avg_prop_rmse_sol6 = mean(prop_rmse_sol6) #$76 -> really bad ughhh
```

### SOL (2021)

##### Yearly seasonality = 20

```{r}
getSymbols.yahoo("SOL-USD", periodicity = "daily", env = .GlobalEnv,from="2021-02-10")

sol = as.data.frame(`SOL-USD`)
head(sol)
tail(sol)

sol$date = rownames(sol)

sol_ts = ts(sol, start =c(year(as.Date("2021-02-10")),
                          as.numeric(format(as.Date("2021-02-10"), "%j")))
            , frequency = 365)
head(sol_ts)

sol_adj = sol_ts[,"SOL-USD.Adjusted"]


# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Solana (2021).pdf", 
#     width=8,  height = 8)
par(mfrow = c(2,1))
plot(sol_adj, xlab = "Time", ylab = "USD", col = "forestgreen",
     main = "Solana Historical Price (Daily)")

plot(sol_ts[,"SOL-USD.Volume"], xlab = "Time", ylab = "Volume", col = "forestgreen",
     main = "Solana Historical Volume (Daily)")
dev.off()


#Rename to satisfy ds and y requirement 
new_sol = sol[,c("SOL-USD.Adjusted", "date")]
colnames(new_sol) = c("y", "ds")
tail(new_sol)
head(new_sol)

#add holidays
suppressMessages(library(dplyr)) 
hld = tis::holidays(c(2021:2022))
us_holidays = data_frame(holiday = "All holidays", 
                         ds =as.Date(as.character(hld), format =  "%Y%m%d"))

prophet_mod_sol7 = prophet(new_sol, daily.seasonality = T,  yearly.seasonality = 20 , seasonality.mode = "multiplicative")

##Forecast
future_sol7 = make_future_dataframe(prophet_mod_sol7, periods = 60, freq = "day")
head(future_sol7)
tail(future_sol7)
sol_price_fc7 = predict(prophet_mod_sol7, future_sol7)
#sol_price_fc2[c(741:802),]

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Solana Forecast.pdf",
    width=8,  height = 8)
plot(prophet_mod_sol7, sol_price_fc7) +add_changepoints_to_plot(prophet_mod_sol7)
dev.off()

##All components
pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Solana Components.pdf",
    width=8,  height = 8)
prophet_plot_components(prophet_mod_sol7, sol_price_fc7) 
dev.off()
#Tuesday and Monday dip, peak on wednesday, sat and sun
 df_cv_sol7 = cross_validation(prophet_mod_sol7, initial = 366, period = 5, 
                                horizon = 20,units = 'days') # 11 fc
cv_sol_perf7 = performance_metrics(df_cv_sol7)
prop_rmse_sol7 = cv_sol_perf7[,3]

head(cv_sol_perf7) #RMSEs smaller: first 2 days are only off by $7

avg_prop_rmse_sol7 = mean(prop_rmse_sol7) # reduced to on avg $23 -> best one!!!
```

##### Model comparison

```{r}
sol_rmse_df = data.frame("Average RMSE" = c(avg_prop_rmse_sol, avg_prop_rmse_sol2,
                                            mean(prop_rmse_sol31), mean(prop_rmse_sol4),
                                            mean(prop_rmse_sol5), 
                                            sqrt(mean(nnar_cv^2, na.rm = T)),
                                            avg_prop_rmse_sol6,
                                            avg_prop_rmse_sol7 ),
                         row.names = c("daily seasonality", 
                                       " all seasonality ", 
                                       "daily + weekly + monthly seasonality w/o multiplicative",
                                       "default model", 
                            "fourier for yearly seasonality",
                            "NNAR",
                            "Yearly seaosnality = 20",
                            "Yearly seaosnality = 20, SOL only from 2021"
                            ))
print(sol_rmse_df)
#last one is best one

head(cv_sol_perf)
head(cv_sol_perf2)
head(cv_sol_perf31)
head(cv_sol_perf4)
head(cv_sol_perf5)
head(cv_sol_perf6)
head(cv_sol_perf7)
```

#### Best model

```{r}
getSymbols.yahoo("SOL-USD", periodicity = "daily", env = .GlobalEnv,from="2021-02-10")

sol = as.data.frame(`SOL-USD`)
head(sol)
tail(sol)

sol$date = rownames(sol)

sol_ts = ts(sol, start =c(year(as.Date("2021-02-10")),
                          as.numeric(format(as.Date("2021-02-10"), "%j")))
            , frequency = 365)
head(sol_ts)

sol_adj = sol_ts[,"SOL-USD.Adjusted"]


# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Solana (2021).pdf", 
#     width=8,  height = 8)
par(mfrow = c(2,1))
plot(sol_adj, xlab = "Time", ylab = "USD", col = "forestgreen",
     main = "Solana Historical Price (Daily)")

plot(sol_ts[,"SOL-USD.Volume"], xlab = "Time", ylab = "Volume", col = "forestgreen",
     main = "Solana Historical Volume (Daily)")
dev.off()


#Rename to satisfy ds and y requirement 
new_sol = sol[,c("SOL-USD.Adjusted", "date")]
colnames(new_sol) = c("y", "ds")
tail(new_sol)
head(new_sol)

#add holidays
suppressMessages(library(dplyr)) 
hld = tis::holidays(c(2021:2022))
us_holidays = data_frame(holiday = "All holidays", 
                         ds =as.Date(as.character(hld), format =  "%Y%m%d"))

prophet_mod_sol7 = prophet(new_sol, daily.seasonality = T,  yearly.seasonality = 20 , seasonality.mode = "multiplicative")

##Forecast
future_sol7 = make_future_dataframe(prophet_mod_sol7, periods = 60, freq = "day")
head(future_sol7)
tail(future_sol7)
sol_price_fc7 = predict(prophet_mod_sol7, future_sol7)
#sol_price_fc2[c(741:802),]

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Solana Forecast.pdf",
    width=8,  height = 8)
plot(prophet_mod_sol7, sol_price_fc7, xlabel = "Time", ylabel = "Price") +add_changepoints_to_plot(prophet_mod_sol7)
dev.off()


```

## Terra

```{r}
getSymbols.yahoo("LUNA1-USD", periodicity = "daily", env = .GlobalEnv)

luna = as.data.frame(`LUNA1-USD`)
head(luna)
tail(luna)

luna$date = rownames(luna)

luna_ts = ts(luna, start = c(year(as.Date("2019-07-26")),
                          as.numeric(format(as.Date("2019-07-26"), "%j"))), frequency = 365)
head(luna_ts)

luna_adj = luna_ts[,"LUNA1-USD.Adjusted"]

plot_ly(x=~luna$date, y=~luna$`LUNA1-USD.Adjusted`, type = "scatter", mode = "lines",
        line = list(color = "forestgreen"),
        name = "LUNA Price") %>% layout("Terra Historical Price", showlegend = T,
                                       xaxis = list(title="Date"),
                                       yaxis = list(title="USD"))
```

#### Price Prediction

```{r}
#Rename to satisfy ds and y requirement 
new_luna = luna[,c("LUNA1-USD.Adjusted", "date")]
colnames(new_luna) = c("y", "ds")
tail(new_luna)
head(new_luna)


```

##### entire length: Fit model: yearly + daily seasonality (prophet suggested)

```{r}
new_luna = luna[,c("LUNA1-USD.Adjusted", "date")]
colnames(new_luna) = c("y", "ds")
tail(new_luna)
head(new_luna)

prophet_mod_luna = prophet(new_luna, seasonality.mode = "multiplicative", daily.seasonality = T, yearly.seasonality = T)
##Forecast
future_luna = make_future_dataframe(prophet_mod_luna, periods = 60, freq = "day")
head(future_luna)
tail(future_luna)
luna_price_fc = predict(prophet_mod_luna, future_luna)
plot(prophet_mod_luna, luna_price_fc) +add_changepoints_to_plot(prophet_mod_luna)
##Forecast
future_luna = make_future_dataframe(prophet_mod_luna, periods = 60, freq = "day")
head(future_luna)
tail(future_luna)
luna_price_fc = predict(prophet_mod_luna, future_luna)
plot(prophet_mod_luna, luna_price_fc) +add_changepoints_to_plot(prophet_mod_luna)
##All components
prophet_plot_components(prophet_mod_luna, luna_price_fc) 
#Peak on wednesday and dip on friday 
df_cv_luna = cross_validation(prophet_mod_luna, initial = 365, period = 60, horizon =140,units = 'days') #9 fc
cv_luna_perf = performance_metrics(df_cv_luna)
prop_rmse_luna = cv_luna_perf[,3]
mean(prop_rmse_luna) #on avg off by $30

```

##### 2021 only: Fit model: same model

```{r}
#Only from 2021
getSymbols.yahoo("LUNA1-USD", periodicity = "daily", env = .GlobalEnv, from = "2021-01-01")

luna2 = as.data.frame(`LUNA1-USD`)
head(luna2)
tail(luna2)

luna2$date = rownames(luna2)

luna2_ts = ts(luna2, start = c(year(as.Date("2021-01-01")),
                          as.numeric(format(as.Date("2021-01-01"), "%j"))), frequency = 365)
head(luna2_ts)

luna2_adj = luna2_ts[,"LUNA1-USD.Adjusted"]

new_luna2 = luna2[,c("LUNA1-USD.Adjusted", "date")]
colnames(new_luna2) = c("y", "ds")
tail(new_luna2)
head(new_luna2)

prophet_mod_luna2 = prophet(new_luna2, seasonality.mode = "multiplicative", 
                            daily.seasonality = T, yearly.seasonality = T)
##Forecast
future_luna2 = make_future_dataframe(prophet_mod_luna2, periods = 60, freq = "day")
luna_price_fc2 = predict(prophet_mod_luna2, future_luna2)
plot(prophet_mod_luna2, luna_price_fc2) +add_changepoints_to_plot(prophet_mod_luna2)

##All components
prophet_plot_components(prophet_mod_luna2, luna_price_fc2) 
#Peak on wednesday and dip on friday 

#Cross validation: 
df_cv_luna2 = cross_validation(prophet_mod_luna2, initial = 366, period = 10, 
                                horizon = 30,units = 'days') #9 fc
cv_luna_perf2 = performance_metrics(df_cv_luna2)
prop_rmse_luna2 = cv_luna_perf2[,3]
mean(prop_rmse_luna2) #on average off by $20 -> much better than the entire length


```

##### 2021 with customized yearly

```{r}

prophet_mod_luna3 = prophet(new_luna2, seasonality.mode = "multiplicative", 
                            daily.seasonality = T, yearly.seasonality = 20)
##Forecast
future_luna3 = make_future_dataframe(prophet_mod_luna3, periods = 60, freq = "day")
luna_price_fc3 = predict(prophet_mod_luna3, future_luna3)
plot(prophet_mod_luna3, luna_price_fc3) +add_changepoints_to_plot(prophet_mod_luna3)

##All components
prophet_plot_components(prophet_mod_luna3, luna_price_fc3) 
#Peak on wednesday and dip on friday 

#Cross validation: 
df_cv_luna3 = cross_validation(prophet_mod_luna3, initial = 366, period = 10, horizon = 30,units = 'days') #9 fc
cv_luna_perf3 = performance_metrics(df_cv_luna3)
prop_rmse_luna3 = cv_luna_perf3[,3]
mean(prop_rmse_luna3) #on average $23, not as good as default yearly seasonality
```

##### without daily seasonality

```{r}
prophet_mod_luna4 = prophet(new_luna2, seasonality.mode = "multiplicative", 
                            daily.seasonality = F, yearly.seasonality = T)
##Forecast
future_luna4 = make_future_dataframe(prophet_mod_luna4, periods = 60, freq = "day")
luna_price_fc4 = predict(prophet_mod_luna4, future_luna4)
plot(prophet_mod_luna4, luna_price_fc4) +add_changepoints_to_plot(prophet_mod_luna4)

##All components
prophet_plot_components(prophet_mod_luna4, luna_price_fc4) 
#Peak on wednesday and dip on friday 

#Cross validation: 
df_cv_luna4 = cross_validation(prophet_mod_luna4, initial = 366, period = 10, horizon = 30,units = 'days') #9 fc
cv_luna_perf4 = performance_metrics(df_cv_luna4)
prop_rmse_luna4 = cv_luna_perf4[,3]
mean(prop_rmse_luna4) #rmse = 21.95774
```

##### Model comparison

```{r}
luna_rmse_df = data.frame("Average RMSE" = c(mean(prop_rmse_luna),
                                            mean(prop_rmse_luna2),
                                            mean(prop_rmse_luna3), 
                                            mean(prop_rmse_luna4)),
                         row.names = c("yearly + daily seasonality", 
                                       " all seasonality + Only from 2021", 
                                       "2021 with customized yearly",
                                       "yearly seasonality"))

print(luna_rmse_df)
head(cv_luna_perf)
head(cv_luna_perf2)
head(cv_luna_perf3)
head(cv_luna_perf4)
```

Best model is only using 2021 onward data + all seasonality

#### Best model

```{r}
#Only from 2021
getSymbols.yahoo("LUNA1-USD", periodicity = "daily", env = .GlobalEnv, from = "2021-01-01")

luna2 = as.data.frame(`LUNA1-USD`)
head(luna2)
tail(luna2)

luna2$date = rownames(luna2)

luna2_ts = ts(luna2, start = c(year(as.Date("2021-01-01")),
                          as.numeric(format(as.Date("2021-01-01"), "%j"))), frequency = 365)
head(luna2_ts)

luna2_adj = luna2_ts[,"LUNA1-USD.Adjusted"]

new_luna2 = luna2[,c("LUNA1-USD.Adjusted", "date")]
colnames(new_luna2) = c("y", "ds")
tail(new_luna2)
head(new_luna2)

prophet_mod_luna2 = prophet(new_luna2, seasonality.mode = "multiplicative", 
                            daily.seasonality = T, yearly.seasonality = T)
##Forecast
future_luna2 = make_future_dataframe(prophet_mod_luna2, periods = 60, freq = "day")
luna_price_fc2 = predict(prophet_mod_luna2, future_luna2)
pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/LUNA Forecast.pdf",
    width=8,  height = 8)
plot(prophet_mod_luna2, luna_price_fc2) +add_changepoints_to_plot(prophet_mod_luna2)
dev.off()
##All components
prophet_plot_components(prophet_mod_luna2, luna_price_fc2) 
#Peak on wednesday and dip on friday 

```

## Cardano

```{r}

getSymbols.yahoo("ADA-USD", periodicity = "daily", env = .GlobalEnv)
ada = as.data.frame(`ADA-USD`)
head(ada)
tail(ada)

ada$date = rownames(ada)

ada_ts = ts(ada, start = c(year(as.Date("2017-11-09")),
                          as.numeric(format(as.Date("2017-11-09"), "%j"))), frequency = 365)
head(ada_ts)

ada_adj = ada_ts[,"ADA-USD.Adjusted"]

plot_ly(x=~ada$date, y=~ada$`ADA-USD.Adjusted`, type = "scatter", mode = "lines",
        line = list(color = "forestgreen"), name = "Cardano Price") %>% layout("Cardano Historical Price", showlegend = T,
                                       xaxis = list(title="Date"),
                                       yaxis = list(title="USD"))



```

#### Price Prediction

##### Default model

```{r}
#Rename to satisfy ds and y requirement 
new_ada = ada[,c("ADA-USD.Adjusted", "date")]
colnames(new_ada) = c("y", "ds")
tail(new_ada)
head(new_ada)

#Fit default model
prophet_mod_ada = prophet(new_ada)


##Forecast
future_ada = make_future_dataframe(prophet_mod_ada, periods = 60, freq = "day")
head(future_ada)
tail(future_ada)
ada_price_fc = predict(prophet_mod_ada, future_ada)
plot(prophet_mod_ada, ada_price_fc) +add_changepoints_to_plot(prophet_mod_ada)

##All components
prophet_plot_components(prophet_mod_ada, ada_price_fc) 
#Peaks are during the weekend and dips are Tuesday and Friday

#Cross validation: 
df_cv_ada = cross_validation(prophet_mod_ada, initial = 366, period = 90, horizon = 180,
                               units = 'days')
cv_ada_perf = performance_metrics(df_cv_ada)
prop_rmse_ada = cv_ada_perf[,3]

avg_prop_rmse_ada = mean(prop_rmse_ada)

head(cv_ada_perf) #RMSEs are huge :(((

```

##### Add daily seasonality + fourier order

Adding fourier order will allow seasonality to change quickly

```{r}
#Fit model
temp_mod = prophet(seasonality.mode = "multiplicative", daily.seasonality = F)
prophet_mod_ada2 = add_seasonality(temp_mod, name='daily',
                                   fourier.order=10, period = 365)
prophet_mod_ada2 = fit.prophet(prophet_mod_ada2, new_ada)

##Forecast
future_ada2 = make_future_dataframe(prophet_mod_ada2, periods = 60, freq = "day")
head(future_ada2)
tail(future_ada2)
ada_price_fc2 = predict(prophet_mod_ada2, future_ada2)
plot(prophet_mod_ada2, ada_price_fc2) +add_changepoints_to_plot(prophet_mod_ada2)

##All components
prophet_plot_components(prophet_mod_ada2, ada_price_fc2) 
#Peaks are on thursday, sat and sun and dip is Tuesday 

#Cross validation: 
df_cv_ada2 = cross_validation(prophet_mod_ada2, initial = 366, period = 90, horizon = 180,
                               units = 'days')
cv_ada_perf2 = performance_metrics(df_cv_ada2)
prop_rmse_ada2 = cv_ada_perf2[3]
avg_prop_rmse_ada2 = mean(prop_rmse_ada2$rmse)
head(cv_ada_perf2) #RMSEs are significantly reduced, off by 18 cents instead of 30 cents

```

##### Customize yearly seasonality

```{r}
#Fit default model
prophet_mod_ada3 = prophet(new_ada, seasonality.mode = "multiplicative", 
                           daily.seasonality = T, yearly.seasonality = 20)


##Forecast
future_ada3 = make_future_dataframe(prophet_mod_ada3, periods = 60, freq = "day")
ada_price_fc3 = predict(prophet_mod_ada3, future_ada3)
plot(prophet_mod_ada3, ada_price_fc3) +add_changepoints_to_plot(prophet_mod_ada3)

##All components
prophet_plot_components(prophet_mod_ada3, ada_price_fc3) 
#Peaks are during the weekend and dips are Tuesday and Friday

#Cross validation: 
df_cv_ada3 = cross_validation(prophet_mod_ada3, initial = 366, period = 90, horizon = 180,
                               units = 'days')
cv_ada_perf3 = performance_metrics(df_cv_ada3)
prop_rmse_ada3 = cv_ada_perf3[,3]
avg_prop_rmse_ada3 = mean(prop_rmse_ada3)
head(cv_ada_perf3)
```

##### Add daily seasonality + fourier order + holiday

```{r}
#add holidays
suppressMessages(library(dplyr)) 
hld = tis::holidays(c(2017:2022))
us_holidays = data_frame(holiday = "All holidays", 
                         ds =as.Date(as.character(hld), format =  "%Y%m%d"))
#Fit model
temp_mod4 = prophet(seasonality.mode = "multiplicative", daily.seasonality = T, holidays=us_holidays)
prophet_mod_ada4 = add_seasonality(temp_mod4, name='monthly', fourier.order=10, period = 365)
prophet_mod_ada4 = fit.prophet(prophet_mod_ada4, new_ada)

##Forecast
future_ada4 = make_future_dataframe(prophet_mod_ada4, periods = 60, freq = "day")
head(future_ada4)
tail(future_ada4)
ada_price_fc4 = predict(prophet_mod_ada4, future_ada4)
plot(prophet_mod_ada4, ada_price_fc4) +add_changepoints_to_plot(prophet_mod_ada4)

##All components
prophet_plot_components(prophet_mod_ada4, ada_price_fc4) 
#Peaks are on thursday, saturday and sunday, and dip is Tuesday 

#Cross validation: 
df_cv_ada4 = cross_validation(prophet_mod_ada4, initial = 366, period = 90, horizon = 180,
                               units = 'days')
cv_ada_perf4 = performance_metrics(df_cv_ada4)
prop_rmse_ada4 = cv_ada_perf4[,3]
avg_prop_rmse_ada4 = mean(prop_rmse_ada4)
head(cv_ada_perf4) #RMSEs are worse with holiday effects

```

##### Only 2021 + customize yearly seasonality

```{r}
getSymbols.yahoo("ADA-USD", periodicity = "daily", env = .GlobalEnv,
                 from = "2020-12-23")

ada2 = as.data.frame(`ADA-USD`)
head(ada2)
tail(ada2)

ada2$date = rownames(ada2)

ada2_ts = ts(ada2, start = c(year(as.Date("2020-12-23")),
                          as.numeric(format(as.Date("2020-12-23"), "%j"))), frequency = 365)
head(ada2_ts)

ada2_adj = ada2_ts[,"ADA-USD.Adjusted"]

new_ada2 = ada2[,c("ADA-USD.Adjusted", "date")]
colnames(new_ada2) = c("y", "ds")

#Fit model
prophet_mod_ada5 = prophet(new_ada2, seasonality.mode = "multiplicative", 
                           daily.seasonality = T, yearly.seasonality = 20)


##Forecast
future_ada5 = make_future_dataframe(prophet_mod_ada5, periods = 60, freq = "day")
ada_price_fc5 = predict(prophet_mod_ada5, future_ada5)

# pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Cardano Fc(ST).pdf",
#     width=8,  height = 8)
# plot(prophet_mod_ada5, ada_price_fc5) +add_changepoints_to_plot(prophet_mod_ada5)

##All components
prophet_plot_components(prophet_mod_ada5, ada_price_fc5) 
#Peaks are during the weekend and dips are Tuesday and Friday

#Cross validation: 
df_cv_ada5 = cross_validation(prophet_mod_ada5, initial = 366, period = 10,
                              horizon = 20,
                               units = 'days')
cv_ada_perf5 = performance_metrics(df_cv_ada5)
prop_rmse_ada5 = cv_ada_perf5[,3]
avg_prop_rmse_ada5 = mean(prop_rmse_ada5)
head(cv_ada_perf5)

```

##### 2021 + default

```{r}
#Fit model
prophet_mod_ada6 = prophet(new_ada2, seasonality.mode = "multiplicative")


##Forecast
future_ada6 = make_future_dataframe(prophet_mod_ada6, periods = 60, freq = "day")
ada_price_fc6 = predict(prophet_mod_ada6, future_ada6)

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Cardano Fc(Avg).pdf",
    width=8,  height = 8)
plot(prophet_mod_ada6, ada_price_fc6) +add_changepoints_to_plot(prophet_mod_ada6)

##All components
prophet_plot_components(prophet_mod_ada6, ada_price_fc6) 
#Peaks are during the weekend and dips are Tuesday and Friday

#Cross validation: 
df_cv_ada6 = cross_validation(prophet_mod_ada6, initial = 366, period = 10,
                              horizon = 20,
                               units = 'days')
cv_ada_perf6 = performance_metrics(df_cv_ada6)
prop_rmse_ada6 = cv_ada_perf6[,3]
avg_prop_rmse_ada6 = mean(prop_rmse_ada6)
head(cv_ada_perf6)
```

##### Model Comparison

```{r}
ada_rmse_df = data.frame("Average RMSE" = c(avg_prop_rmse_ada,
                                            avg_prop_rmse_ada2,
                                            avg_prop_rmse_ada3, 
                                           avg_prop_rmse_ada4,
                                           avg_prop_rmse_ada5,
                                           avg_prop_rmse_ada6),
                         row.names = c("default model", 
                                       "daily seasonality + fourier for yearly", 
                                       "customized yearly",
                                       "daily seasonality + fourier order + holiday",
                                       "Only 2021 + customize yearly seasonality",
                                       "2021 + default"))
print(ada_rmse_df)
# default model using only 2021 is actually the best (on avg), but if we compare
# by horizon, yearly + 2021 is the best,but rmse is still big
# in comparison to the price
head(cv_ada_perf)
head(cv_ada_perf2)
head(cv_ada_perf3)
head(cv_ada_perf4)
head(cv_ada_perf5)
head(cv_ada_perf6)
```

##### Best Model
```{r}
getSymbols.yahoo("ADA-USD", periodicity = "daily", env = .GlobalEnv,
                 from = "2020-12-23")

ada2 = as.data.frame(`ADA-USD`)
head(ada2)
tail(ada2)

ada2$date = rownames(ada2)

ada2_ts = ts(ada2, start = c(year(as.Date("2020-12-23")),
                          as.numeric(format(as.Date("2020-12-23"), "%j"))), frequency = 365)
head(ada2_ts)

ada2_adj = ada2_ts[,"ADA-USD.Adjusted"]

new_ada2 = ada2[,c("ADA-USD.Adjusted", "date")]
colnames(new_ada2) = c("y", "ds")

#Fit model
prophet_mod_ada5 = prophet(new_ada2, seasonality.mode = "multiplicative", 
                           daily.seasonality = T, yearly.seasonality = 20)


##Forecast
future_ada5 = make_future_dataframe(prophet_mod_ada5, periods = 60, freq = "day")
ada_price_fc5 = predict(prophet_mod_ada5, future_ada5)

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/Cardano Fc(ST).pdf",
    width=8,  height = 8)
plot(prophet_mod_ada5, ada_price_fc5, xlabel = "Time", ylabel = "Price")+add_changepoints_to_plot(prophet_mod_ada5)
dev.off()
#Fit model
prophet_mod_ada6 = prophet(new_ada2, seasonality.mode = "multiplicative")


##Forecast
future_ada6 = make_future_dataframe(prophet_mod_ada6, periods = 60, freq = "day")
ada_price_fc6 = predict(prophet_mod_ada6, future_ada6)

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/cardano_lt.pdf",
    width=8,  height = 8)
plot(prophet_mod_ada6, ada_price_fc6, xlabel = "Time", ylabel = "Price") +add_changepoints_to_plot(prophet_mod_ada6)
dev.off()
```



# Rate of Growth: Bitcoin vs Other Assets

```{r}

sp500 = getSymbols.yahoo("^GSPC", periodicity = "daily", env = .GlobalEnv)

sp500 = as.data.frame(GSPC)

head(sp500)

tail(sp500)

getSymbols.yahoo("GC=F", periodicity = "daily", env = .GlobalEnv)

gold = as.data.frame(`GC=F`)

gold = na.omit(gold)

head(gold)

head(btc)

gold_ts = ts(gold, start = year(as.Date("2007-01-02")), frequency = 252)

sp_ts = ts(sp500, start = year(as.Date("2007-01-03")), frequency = 252)

btc_ts = ts(btc, start = year(as.Date("2014-09-17")), frequency = 365)
head(sp_ts)

gold_adj = gold_ts[, "GC=F.Adjusted"]

sp_adj = sp_ts[,"GSPC.Adjusted"]

btc_adj = btc_ts[, "BTC-USD.Adjusted"]


plot(sp_adj)

gold_growth = ((gold_adj - gold_adj[1])/gold_adj[1])*100

norm_gold_growth = (gold_growth - mean(gold_growth))/sd(gold_growth)

sp_growth = ((sp_adj - sp_adj[1])/sp_adj[1])*100

norm_sp_growth = (sp_growth - mean(sp_growth))/sd(sp_growth)

btc_growth = ((btc_adj - btc_adj[1])/btc_adj[1])*100

norm_btc_growth = (btc_growth - mean(btc_growth))/sd(btc_growth)

length(norm_btc_growth)
dim(btc)

sp500$date = rownames(sp500)
head(sp500)

gold$date = rownames(gold)

btc$date = rownames(btc)

grow_plot = plot_ly(x = ~btc$date,y=~norm_btc_growth, type = "scatter", mode = "lines",
        name = "Bitcoin Price Growth", line = list(color = "forestgreen"),
                         text = ~paste("Bitcoin Price Growth:", btc_growth,
                                       "<br> Date:", btc$date),
                         hoverinfo = 'text') %>% 
  add_trace(x = ~sp500$date, y=~norm_sp_growth, type = "scatter", 
            mode = "lines", name = "SP500 Price Growth", line = list(color = "salmon"),text = ~paste("SP500 Price Growth:", sp_growth, "<br> Date:", sp500$date),
                         hoverinfo = 'text')

grow_plot = grow_plot %>% add_trace(x = ~gold$date, y=~norm_gold_growth, type = "scatter", line = list(color = "gold"),
            mode = "lines", name = "Gold Price Growth", 
                         text = ~paste("Gold Price Growth:", gold_growth,
                                       "<br> Date:", gold$date),
                         hoverinfo = 'text')

grow_plot = grow_plot %>% layout(title = list(text='Bitcoin Growth vs Other Asset Classes Growth',titlefont = list(size =30)),
         xaxis = list(title = 'Date', titlefont = list(size =30)),
         yaxis = list(title = 'Growth', titlefont = list(size =30)),
         showlegend=T,margin = list(l=50, r=50, b=100, t=100, pad=4))

grow_plot

#Not in z score
grow_plot2 = plot_ly(x = ~btc$date,y=~btc_growth, type = "scatter", mode = "lines",
        name = "Bitcoin Price Growth", line = list(color = "forestgreen"),
                         text = ~paste("Bitcoin Price Growth:", btc_growth,
                                       "<br> Date:", btc$date),
                         hoverinfo = 'text') %>% 
  add_trace(x = ~sp500$date, y=~sp_growth, type = "scatter", 
            mode = "lines", name = "SP500 Price Growth", line = list(color = "salmon"),text = ~paste("SP500 Price Growth:", sp_growth, "<br> Date:", sp500$date),
                         hoverinfo = 'text')

grow_plot2 = grow_plot2 %>% add_trace(x = ~gold$date, y=~gold_growth, type = "scatter", line = list(color = "gold"),
            mode = "lines", name = "Gold Price Growth", 
                         text = ~paste("Gold Price Growth:", gold_growth,
                                       "<br> Date:", gold$date),
                         hoverinfo = 'text')

grow_plot2 = grow_plot2 %>% layout(title = list(text='Bitcoin Growth vs Other Asset Classes Growth',titlefont = list(size =30)),
         xaxis = list(title = 'Date', titlefont = list(size =30)),
         yaxis = list(title = 'Growth', titlefont = list(size =30)),
         showlegend=T,margin = list(l=50, r=50, b=100, t=100, pad=4))

grow_plot2
```

# Price and Investor Sentiment

## University of Michigan Consumer Sentiment

```{r}
getSymbols.FRED("UMCSENT", env = .GlobalEnv) #Index 1966 Q1 = 100
# umc = read.xls("tbmics.xls", header = T, stringsAsFactors = T, sep = ",")
umc = as.data.frame(UMCSENT)
head(umc)
tail(umc)
umc$date = rownames(umc)

any(is.na(umc$UMCSENT))


umc_ts = ts(umc, start = c(1952,11), end = c(2022,4), frequency = 12)
head(umc_ts)


umc_ts = interpNA(umc_ts, "linear")

umc_sent = umc_ts[,"UMCSENT"]

norm_umc_sent = (umc_sent - mean(umc_sent))/sd(umc_sent)

norm_btc_adj = (btc_adj - mean(btc_adj))/sd(btc_adj)

norm_eth_adj =(eth_adj - mean(eth_adj))/sd(eth_adj)

norm_usdt_adj =(usdt_adj - mean(usdt_adj))/sd(usdt_adj)

norm_sol_adj =(sol_adj - mean(sol_adj))/sd(sol_adj)


sentiment_plot = plot_ly(x=~umc$date, y=~norm_umc_sent, 
                         name = "Consumer Sentiment", 
                         type = "scatter",mode = "line", 
                         text = ~paste("Sentiment Index:", umc_sent,
                                       "<br> Date:", umc$date),
                         hoverinfo = 'text')

sentiment_plot = sentiment_plot %>% 
  add_trace(x=~btc$date, y=~norm_btc_adj, name = "Bitcoin Price", 
            text = ~paste("Bitcoin Price:", btc_adj, "<br>Date:", btc$date),
                         hoverinfo = 'text')

sentiment_plot = sentiment_plot %>% 
  add_trace(x= ~eth$date, y=~norm_eth_adj, name = "Ethereum Price", 
            text = ~paste("Etherum Price:", eth_adj, "<br>Date:", eth$date),
                         hoverinfo = 'text')

# sentiment_plot = sentiment_plot %>% 
#   add_trace(x= ~usdt$date, y=~norm_usdt_adj, name = "Tether Price", 
#             text = ~paste("Tether Price:", usdt_adj, "<br>Date:", usdt$date),
#                          hoverinfo = 'text')

sentiment_plot = sentiment_plot %>% 
  add_trace(x= ~sol$date, y=~norm_sol_adj, name = "Solana Price", 
            text = ~paste("Solana Price:", sol_adj, "<br>Date:", sol$date),
                         hoverinfo = 'text')


sentiment_plot = sentiment_plot %>% layout(
  title = list(text='Cryptocurrencies Prices & Consumer Sentiment',titlefont = list(size =30)),
         xaxis = list(title = 'Date', titlefont = list(size =30)),
         yaxis = list(title = 'Scale', titlefont = list(size =30)),
         showlegend=T,margin = list(l=50, r=50, b=100, t=100, pad=4))

sentiment_plot


```

These 2 somewhat move in same direction at a given time interval, ie:
consumers' sentiment is low -\> price drops. There seems to be a small
time lag

### VAR & Granger Test (monthly observation)

#### Bitcoin (monthly)
```{r}
getSymbols.yahoo("BTC-USD", periodicity = "monthly",  
                 env = .GlobalEnv,index.class = "Date")
m_btc = `BTC-USD`
m_btc = as.data.frame(m_btc)
m_btc$date = rownames(m_btc)
m_btc_ts = ts(m_btc, start = c(2014,10), frequency = 12)
m_btc_adj = m_btc_ts[,"BTC-USD.Adjusted"]
tail(m_btc_adj)
```


```{r}

#Align dates 
df = cbind(m_btc_adj, umc_sent)
combine_date = na.omit(df)
tail(combine_date)
head(combine_date)
any(is.na(combine_date))
umc_btc_df = as.data.frame(combine_date)
dim(umc_btc_df)
VARselect(umc_btc_df, lag.max = 10)

var_mod = VAR(umc_btc_df, 8)

summary(var_mod)

var_mod$datamat

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/irf.pdf",
    width=8,  height = 8)
plot(irf(var_mod, impulse = "umc_sent", response = "m_btc_adj"), 
     main = "Impulse Response Function from Consumer Sentiment", ylab = "Bitcoin Price")
dev.off()

#Interesting result: coef for umc_sent at lag 7 is statistically significant, which agrees with the assumption from the plot that there is a lag between consumer sentiment and price of bitcoin; it takes 7 lags for sentiment to start affecting price. Monthly observation, sentiment would take effect in around 6-7 months?

#Granger test
grangertest(umc_btc_df[,"m_btc_adj"] ~ umc_btc_df[,"umc_sent"], order = 7)
#Reject the null -> sentiment granger causes price movement 

var_fc = predict(var_mod)

pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/VAR FC Plot.pdf",
    width=8,  height = 8)
plot(var_fc, main = c("Bitcoin Price Forecast", "Consumer Sentiment Forecast"),
     ylab = c("Price", "Index"), xlab = "Time") 
legend("topleft",legend = c("Actual", "Forecast", "Interval Forecast"),
       text.col = c("black", "blue", "red"),cex = 0.6)
#Sentiment is going down and bitcoin price will go up
dev.off()

##Validation
set.seed(3773)
train_var = window(combine_date, end = c(2019,10))
test_var = window(combine_date, start = c(2019,11))
var_train_mod = VAR(as.data.frame(train_var), p=7)
var_train_pred = predict(var_train_mod, n.ahead = length(test_var))
var_train_pred$fcst$m_btc_adj[,"fcst"]
accuracy(var_train_pred$fcst$m_btc_adj[,"fcst"], test_var[,1])

```

Bitcoin is considered inferior good compared to traditional stock
marktet -\> when the economy is not doing well, consumers move towards
inferior goods instead. Futhermore, Bitcoin as well as other
cryptocurrecies, have no barrier to entry and no limit on what they need
to start with -\> consumers who want to invest don't want to go all in
on one stock -\> cryptocurrencies are the perfect choice

## VIX

```{r}
getSymbols.yahoo("^VIX",  periodicity = "daily", env = .GlobalEnv,
                 from = "2014-09-17")


vix = as.data.frame(VIX)

vix$date = as.Date(rownames(vix))

head(vix) 
tail(vix)

vix_ts = ts(VIX, start = c(year(as.Date("2014-09-17"))), frequency = 250)


vix_adj = vix_ts[,"VIX.Adjusted"]

plot(vix_adj)

vix_adj = interpNA(vix_adj, "linear")

btc_return = diff(log(btc_adj))
btc_return2 = btc_return^2

#Match dates VIX and BTC
small_vix = vix[,c("VIX.Adjusted", "date")]

test_date = seq(as.Date("2014-09-17"), as.Date("2022-06-05"), 
                length=length(btc$date))

# Set date to index
suppressMessages(library(padr))

head(small_vix)

#Insert weekend dates 
small_vix2 = pad(small_vix, start_val = as.Date("2014-09-17"), 
                 end_val = as.Date("2022-06-05"))

head(small_vix2)
small_vix2_ts = ts(small_vix2, start = c(year(as.Date("2014-09-17"))), 
                   frequency = 365)
vix_adj2 = small_vix2_ts[,"VIX.Adjusted"]
#Interpolate NA
vix_adj22 = interpNA(vix_adj2, "linear")
#The last 2 days (as of 6/5) are weekend -> cant interpolate -> drop
coin_btc = btc_adj[1:2817]
coin_vix = vix_adj22[1:2817]

#vix_adj22 now has the same length as btc_adj!!!

#Cointegration: 

# Johanson test
x=cbind(coin_vix, coin_btc)
m1=ar(x)
m1$order
m2=ca.jo(x,K=34)
summary(m2)

# ADF test: null: there is no cointegration
wt=coin_vix-0.000163689*coin_btc
pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/WT.pdf",
    width=8,  height = 8)
plot(wt,type='l',ylab="Combination=VIX-0.000163689*BTC",
     main = "Co-integration Relation")
abline(h=mean(wt, na.rm = T),col="red")
legend("topright",c("wt","E[wt] = 16.13"),fill=c("black","red"), cex = 0.7)
dev.off()
tsdisplay(wt)

xtable(adf.test(wt))
#reject the null -> reject that there is no stationary/cointegration 
#-> there is cointegration -> these 2 move together

#Normalize btc_price return and vix_adj



norm_vix_adj = (vix_adj- mean(vix_adj))/sd(vix_adj)

norm_btc_adj = (btc_adj- mean(btc_adj))/sd(btc_adj)


vix_plot = plot_ly(x=~btc$date, y=~norm_btc_adj, type = 'scatter', mode = 'lines',
        line = list(color = 'darkslategray', width = 2),
                  name = "Bitcoin Price",text = ~paste("Bitcoin Price:", btc_adj,
                                       "<br> Date:", btc$date),hoverinfo = 'text')

vix_plot = vix_plot %>% add_trace(x=~vix$date, y=~norm_vix_adj, name = 'VIX Index', 
                                  line = list(color = 'salmon', width = 1.5, dash = 'dot'), 
                                  text = ~paste("VIX Index:", vix_adj,
                                       "<br> Date:", vix$date),
                         hoverinfo = 'text')
  
vix_plot = vix_plot %>% layout(title = list(text='Fear Index and Bitcoin Price Volatility',titlefont = list(size =30)),
         xaxis = list(title = 'Date', titlefont = list(size =30)),
         yaxis = list(title = 'Scale', titlefont = list(size =30)),
         showlegend=T, legend = list(font = list(size = 30)), 
margin = list(l=50, r=50, b=100, t=100, pad=4))

vix_plot

```
## SP500
```{r}
getSymbols.yahoo("^GSPC", periodicity = "daily", env = .GlobalEnv,
                         from = "2014-09-17")

sp500 = as.data.frame(GSPC)
sp500$date = as.Date(rownames(sp500))
head(sp500)

small_sp = sp500[,c("GSPC.Adjusted", "date")]

test_date = seq(as.Date("2014-09-17"), as.Date("2022-06-05"), 
                length=length(btc$date))

# Set date to index
suppressMessages(library(padr))

#Insert weekend dates 
small_sp2 = pad(small_sp, start_val = as.Date("2014-09-17"), 
                 end_val = as.Date("2022-06-05"))

small_sp2_ts = ts(small_sp2, start = c(year(as.Date("2014-09-17"))), 
                   frequency = 365)
sp_adj2 = small_sp2_ts[,"GSPC.Adjusted"]
#Interpolate NA
sp_adj22 = interpNA(sp_adj2, "linear")
#The last 2 days (as of 6/5) are weekend -> cant interpolate -> drop
coin_sp = sp_adj22[1:2817]
#Cointegration: 
# Johanson test
x=cbind(coin_sp, coin_btc)
m1=ar(x)
m1$order
m2=ca.jo(x,K=34)
summary(m2)

# ADF test: null: there is no cointegration
wt=coin_sp-0.0524325*coin_btc
pdf("/Users/summerletan/Documents/UCLA /Econ 199/Plots/coin_sp.pdf",
    width=8,  height = 8)
plot(wt,type='l',ylab="Combination=SP500-0.0524325*BTC",
     main = "Co-integration Relation", ylim = c(500, 4000))
abline(h=mean(wt, na.rm = T),col="red")
legend("topright",c("wt","E[wt] = 2229.001"),fill=c("black","red"), cex = 0.7)
dev.off()
tsdisplay(wt)

adf.test(wt) 

#Reject the null, btc and sp follow same trned
```


# Adoption curve

```{r}
all_adoption = read.csv("all-adoption.csv", header = T, sep = ",", stringsAsFactors = T)

#rename

colnames(all_adoption) = c("Technology", "Code", "Year", "Rate")

head(all_adoption)

unique(all_adoption$Technology)

which(all_adoption$Technology == "Cellular phone")

which(all_adoption$Technology == "Smartphone usage")

which(all_adoption$Technology == "Computer")

which(all_adoption$Technology == "Internet")

which(all_adoption$Technology == "Social media usage")

which(all_adoption$Technology == "Tablet")

adopt_plot = plot_ly(all_adoption, y = ~all_adoption[163:188, 4],
        x = ~all_adoption$Year[163:188], type = 'scatter', mode = 'lines+markers',
         name = "Cellular phone") %>% 
  add_trace(x= ~all_adoption$Year[1082:1090],y=all_adoption[1082:1090, 4], 
            name = 'Smartphone usage', mode = 'lines+markers') %>% 
  add_trace(x= ~all_adoption$Year[236:246],y=all_adoption[236:246, 4],
            name = 'Computer', mode = 'lines+markers') %>% 
  add_trace(x= ~all_adoption$Year[619:642],y=all_adoption[619:642, 4],
            name = 'Internet', mode = 'lines+markers')%>% 
  add_trace(x= ~all_adoption$Year[1091:1105],y=all_adoption[1091:1105, 4],
            name = 'Social media usage', mode = 'lines+markers')%>% 
  add_trace(x= ~all_adoption$Year[1214:1221],y=all_adoption[1214:1221, 4],
            name = 'Tablet', mode = 'lines+markers')

adopt_plot = adopt_plot %>% layout(title = 'Technology Adoption Rate',
         xaxis = list(title = 'Year'),
         yaxis = list(title = 'Adoption Rate'),
         showlegend=T)

adopt_plot 

```

# Wealth level

## Convert btc to quarter

Note: re scale data

```{r}
#Convert to quarterly obs using zoo: problem: too many Q, need to average for each duplicate
#First extract to month
m_btc$month =  as.yearmon(m_btc$date)
head(m_btc)

#Then convert month to quarter
m_btc$qtr = as.yearqtr(m_btc$month)

new_m_btc = m_btc[, c("BTC-USD.Adjusted", "qtr")]
new_m_btc

#Mean for each quarter
q_btc = aggregate(new_m_btc$`BTC-USD.Adjusted`,by=list(new_m_btc$qtr),
                  data=new_m_btc,FUN=mean)
head(q_btc)
tail(q_btc)
colnames(q_btc) = c("quarter", "average_price")
q_btc_ts = ts(q_btc, frequency = 4, start = c(2014,4), end = c(2021,4))
q_btc_p = q_btc_ts[, "average_price"]
q_btc_return= diff(log(q_btc_p))
length(q_btc_return)
avg_return = mean(q_btc_return)
avg_return
sd(q_btc_return)
```

## Data: Wealth level (in millions)

Change in wealth level by change in equities, crypto return By
percentile

```{r}
wealth_lv = read.csv("dfa-networth-levels.csv", header = T, sep = ',', 
                     stringsAsFactors = T)
head(wealth_lv)

#Rename
names(wealth_lv)[names(wealth_lv)
                 == "Corporate.equities.and.mutual.fund.shares"] = "Equities"

#Drop string variables
new_wealth = wealth_lv[,-c(1,2)]

#Pre process (normalize)
std_wealth = scale(new_wealth, center = T, scale = T)

head(std_wealth)

std_wealth_df=as.data.frame(std_wealth)

std_wealth_df$Date = wealth_lv$Date

std_wealth_df$Category = wealth_lv$Category

head(std_wealth_df)
```

## Top 1%


### Data wrangling

```{r}
set.seed(3773)
wealth_lv_top1 = std_wealth_df[std_wealth_df$Category == "Top1",]
head(wealth_lv_top1)

wealth_lv_top1 = wealth_lv_top1 %>% tidyr::spread(Category, Net.worth)

which(wealth_lv_top1$Date== "2015:Q1")
which(wealth_lv_top1$Date== "2021:Q4")

small_wealth_lv_top1 = wealth_lv_top1[103:130, ]

q_btc_return = as.numeric(q_btc_return)
small_wealth_lv_top1$BTC = q_btc_return
head(small_wealth_lv_top1)

x =  as.matrix(small_wealth_lv_top1[,-which(names(small_wealth_lv_top1) 
                                            %in% c("Top1", "Date"))])
y = small_wealth_lv_top1$Top1
train = sample(1:nrow(x), nrow(x)*.75) 
test = (-train)

```



### Select variables

#### Ridge

```{r}
set.seed(3773)
mod_ridge = cv.glmnet( x = x, y=y, alpha = 0, nfolds = length(y))

vip(mod_ridge, num_features = 9, geom = "point")

#Another plot
plot(mod_ridge$glmnet.fit, "lambda", label=TRUE)
legend("bottomright", lwd = 1, col = 1:3, 
       legend = colnames(small_wealth_lv_top1[,-1]), cex = .5)

#Optimal lambda:
mod_ridge$lambda.min

variable_importance_ridge=coef(mod_ridge, s='lambda.min')
ordered_var_imp_ridge=variable_importance_ridge[,1][
  order(-abs(variable_importance_ridge[,1]))]
as.data.frame(ordered_var_imp_ridge[ordered_var_imp_ridge != 0])
imp_vars_ridge=row.names(as.data.frame(ordered_var_imp_ridge[
  ordered_var_imp_ridge != 0]))
imp_vars_ridge=imp_vars_ridge[-which(imp_vars_ridge == "(Intercept)")]
cat(imp_vars_ridge, sep="+")
cat(imp_vars_ridge, sep=", ")
head(imp_vars_ridge)
#Home.mortgages, Consumer.credit, Liabilities, Pension.entitlements, Consumer.durables, Real.estate
var_to_use = small_wealth_lv_top1[,-which(names(small_wealth_lv_top1) 
                                          %in% c("Top1", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
ridge_train 	= train(Top1~Assets+Real.estate+Consumer.durables+
                       Equities+
                       Pension.entitlements+Private.businesses+
                       Other.assets+Liabilities+Home.mortgages+Consumer.credit+
                       Other.liabilities+BTC, 
                     data = small_wealth_lv_top1[train,],
                           method = "ridge",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
ridge_pred = predict(ridge_train, newdata = small_wealth_lv_top1[test,])

# Evaluate performance
ridge_perf=postResample(pred = ridge_pred, obs = small_wealth_lv_top1[test,"Top1"])

ridge_rmse = ridge_perf[1]

```

#### Lasso

```{r}
set.seed(3773)
mod_lasso = cv.glmnet(x = x, y=y, alpha = 1, nfolds = length(y))

vip(mod_lasso, num_features = 9, geom = "point")

#Another plot
plot(mod_lasso$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:13, 
       legend = colnames(small_wealth_lv_top1[,-1]), cex = .4)

#Optimal lambda:
mod_lasso$lambda.min

variable_importance_lasso=coef(mod_lasso, s='lambda.min')
ordered_var_imp_lasso=variable_importance_lasso[,1][
  order(-abs(variable_importance_lasso[,1]))]
as.data.frame(ordered_var_imp_lasso[ordered_var_imp_lasso != 0])
imp_vars_lasso=row.names(as.data.frame(ordered_var_imp_lasso[
  ordered_var_imp_lasso != 0]))
imp_vars_lasso=imp_vars_lasso[-which(imp_vars_lasso == "(Intercept)")]
cat(imp_vars_lasso, sep="+")
cat(imp_vars_lasso, sep=", ")
# only Assets, Equities are important 

var_to_use = small_wealth_lv_top1[,-which(names(small_wealth_lv_top1) 
                                          %in% c("Top1", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
lasso_train 	= train(Top1~Assets+Real.estate+Consumer.durables+
                       Equities+
                       Pension.entitlements+Private.businesses+
                       Other.assets+Liabilities+Home.mortgages+Consumer.credit+
                       Other.liabilities+BTC, 
                     data = small_wealth_lv_top1[train,],
                           method = "lasso",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
lasso_pred = predict(lasso_train, newdata = small_wealth_lv_top1[test,])

# Evaluate performance
lasso_perf=postResample(pred = lasso_pred, obs = small_wealth_lv_top1[test,"Top1"])

lasso_rmse = lasso_perf[1]
```

#### Enet

```{r}
set.seed(3773)
mod_enet = cv.glmnet(x = x, y=y, alpha = 0.5, nfolds = length(y))

vip(mod_enet, num_features = 9, geom = "point")
#Assets, Real estate, Corp equities, private business, other assets seem to be imp

#Another plot
plot(mod_enet$glmnet.fit, "lambda", label=TRUE)
legend("bottomright", lwd = 1, col = 1:13, 
       legend = colnames(small_wealth_lv_top1[,-1]), cex = .3)

#Optimal lambda:
mod_enet$lambda.min

variable_importance_enet=coef(mod_enet, s='lambda.min')
ordered_var_imp_enet=variable_importance_enet[,1][
  order(-abs(variable_importance_enet[,1]))]
as.data.frame(ordered_var_imp_enet[ordered_var_imp_enet != 0])
imp_vars_enet=row.names(as.data.frame(ordered_var_imp_enet[
  ordered_var_imp_enet != 0]))
imp_vars_enet=imp_vars_enet[-which(imp_vars_enet == "(Intercept)")]
cat(imp_vars_enet, sep="+")
cat(imp_vars_enet, sep=", ")
# Assets, Real.estate, Equities, Other.assets, Private.businesses,
# Other.liabilities

var_to_use = small_wealth_lv_top1[,-which(names(small_wealth_lv_top1) 
                                          %in% c("Top1", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
enet_train 	= train(Top1~Assets+Real.estate+Consumer.durables+
                       Equities+
                       Pension.entitlements+Private.businesses+
                       Other.assets+Liabilities+Home.mortgages+Consumer.credit+
                       Other.liabilities+BTC, 
                     data = small_wealth_lv_top1[train,],
                           method = "enet",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
enet_pred = predict(enet_train, newdata = small_wealth_lv_top1[test,])

# Evaluate performance
enet_perf=postResample(pred = enet_pred, obs = small_wealth_lv_top1[test,"Top1"])

enet_rmse = enet_perf[1]
```

#### PCA

```{r}
set.seed(3773)
#Fit model
mod_pcr = pcr(Top1~Assets+Real.estate+Consumer.durables+
                       Equities+
                       Pension.entitlements+Private.businesses+
                       Other.assets+Liabilities+Home.mortgages+Consumer.credit+
                       Other.liabilities+BTC, data= small_wealth_lv_top1,
              scale = T, validation = "CV")
summary(mod_pcr)
#CV error reduced significantly when 2 PCs are used
mod_pcr$loadings #Using 11 PCs will explain 91.7% of variation, however, 
# using 2 PCs would suffice too because the reduction is already significant


#Plot MSE of Components
validationplot(mod_pcr, val.type = "MSEP")
#Smallest MSE is when we use 16 components out of 17

#Test set
mod_pcr_train = pcr(Top1~Assets+Real.estate+Consumer.durables+
                       Equities+
                       Pension.entitlements+Private.businesses+
                       Other.assets+Liabilities+Home.mortgages+Consumer.credit+
                       Other.liabilities+BTC, data= small_wealth_lv_top1[train,]
                    , scale = T, validation = "CV")
summary(mod_pcr_train) #significant reduce in error happens at 2 PCs

#Plot MSE of Components
validationplot(mod_pcr_train, val.type = "MSEP")

pcr_pred = predict(mod_pcr_train, small_wealth_lv_top1[test,],ncomp = 2)

pcr_rmse = sqrt(mean((pcr_pred - small_wealth_lv_top1[test,"Top1"])^2)) 


```

#### Compare models

```{r}
data.frame("RMSEs"=c(ridge_rmse, lasso_rmse, enet_rmse), 
           row.names = c("Ridge", "Lasso", "Enet") )
```

Lasso's rmse is smallest -\> go with Lasso

#### Tree
```{r}
set.seed(1)

cat(names(small_wealth_lv_top1)[!names(small_wealth_lv_top1) %in% c("Date", "Top1")], sep = "+")
tree1 = rpart(Top1~Assets+Real.estate+Consumer.durables+Equities+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, data = small_wealth_lv_top1, subset = train)
rpart.plot::rpart.plot(tree1)

tree2 = tree(Top1~Assets+Real.estate+Consumer.durables+Equities+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, data = small_wealth_lv_top1, subset = train)

# str(small_wealth_lv_top1)

plot(tree2)
text(tree2, pretty = 0)



```


### Model

```{r}
top1_mod = lm(Top1~Assets+Equities,
              data = small_wealth_lv_top1)
summary(top1_mod)

par(mfrow=c(1,2))
plot(x = small_wealth_lv_top1$Assets, y = small_wealth_lv_top1$Top1,
     xlab = "Assets", ylab = "Net Worth", col = "forestgreen")
plot(x = small_wealth_lv_top1$Equities,y = small_wealth_lv_top1$Top1, col = "cornflowerblue",
     xlab = "Corp Equities", ylab = "Net Worth")
```

## Bottom 50%

### Data wrangling

```{r}
set.seed(3773)
head(std_wealth_df)
wealth_lv_bot50 = std_wealth_df[std_wealth_df$Category == "Bottom50",]
head(wealth_lv_bot50)

wealth_lv_bot50 = wealth_lv_bot50 %>% tidyr::spread(Category, Net.worth)

which(wealth_lv_bot50$Date== "2015:Q1")
which(wealth_lv_bot50$Date== "2021:Q4")

small_wealth_lv_bot50 = wealth_lv_bot50[103:130, ]
small_wealth_lv_bot50$BTC = q_btc_return
head(small_wealth_lv_bot50)

x50 =  as.matrix(small_wealth_lv_bot50[,-which(names(small_wealth_lv_bot50) 
                                            %in% c("Bottom50", "Date"))])
y50 = small_wealth_lv_bot50$Bottom50
train = sample(1:nrow(x50), nrow(x50)*.75) 
test = (-train)
```

### Select variables

#### Ridge

```{r, fig.align='center'}
set.seed(3773)
mod_ridge50 = cv.glmnet( x = x50, y=y50, alpha = 0, nfolds = length(y50))

vip(mod_ridge50, num_features = 9, geom = "point")
#Equities, Pensions, Assets, other assets, private business, real estate

#Another plot
plot(mod_ridge50$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:13, 
       legend = colnames(small_wealth_lv_bot50[,-1]), cex = .4)

#Optimal lambda:
mod_ridge50$lambda.min

variable_importance_ridge50=coef(mod_ridge50, s='lambda.min')
ordered_var_imp_ridge50=variable_importance_ridge50[,1][
  order(-abs(variable_importance_ridge50[,1]))]
as.data.frame(ordered_var_imp_ridge50[ordered_var_imp_ridge50 != 0])
imp_vars_ridge50=row.names(as.data.frame(ordered_var_imp_ridge50[
  ordered_var_imp_ridge50 != 0]))
imp_vars_ridge50=imp_vars_ridge50[-which(imp_vars_ridge50 == "(Intercept)")]
cat(imp_vars_ridge50, sep="+")
cat(imp_vars_ridge50, sep=", ")
#Equities, Pension.entitlements, Assets, Other.assets, Real.estate, Consumer.durables, Home.mortgages, Liabilities, Consumer.credit, Private.businesses, Other.liabilities, BTC
var_to_use = small_wealth_lv_bot50[,-which(names(small_wealth_lv_bot50) 
                                          %in% c("Bottom50", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr50 = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
ridge_train50 	= train(Bottom50~Assets+Real.estate+Consumer.durables+
                        Equities+
                        Pension.entitlements+Private.businesses+Other.assets+
                         Liabilities+Home.mortgages+Consumer.credit+
                         Other.liabilities+BTC, 
                     data = small_wealth_lv_bot50[train,],
                           method = "ridge",
                           tuneLength = 25,
                           trControl = trCtr50)

# Predict using the testing data
ridge_pred50 = predict(ridge_train50, newdata = small_wealth_lv_bot50[test,])

# Evaluate performance
ridge_perf50=postResample(pred = ridge_pred50, 
                          obs = small_wealth_lv_bot50[test,"Bottom50"])

ridge_rmse50 = ridge_perf50[1]
```

#### Lasso

```{r}
set.seed(3773)
mod_lasso50 = cv.glmnet( x = x50, y=y50, alpha = 1, nfolds = length(y50))

vip(mod_lasso50, num_features = 9, geom = "point")
#Equities, Assets, real estate, consumer durables, home mortgages

#Another plot
plot(mod_lasso50$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:13, 
       legend = colnames(small_wealth_lv_bot50[,-1]), cex = .4)

#Optimal lambda:
mod_lasso50$lambda.min

variable_importance_lasso50=coef(mod_lasso50, s='lambda.min')
ordered_var_imp_lasso50=variable_importance_lasso50[,1][
  order(-abs(variable_importance_lasso50[,1]))]
as.data.frame(ordered_var_imp_lasso50[ordered_var_imp_lasso50 != 0])
imp_vars_lasso50=row.names(as.data.frame(ordered_var_imp_lasso50[
  ordered_var_imp_lasso50 != 0]))
imp_vars_lasso50=imp_vars_lasso50[-which(imp_vars_lasso50 == "(Intercept)")]
cat(imp_vars_lasso50, sep="+")

cat(imp_vars_lasso50, sep=", ")
#Important variables:
#Equities, Assets, Real.estate, Consumer.durables, Home.mortgages, Other.assets, Other.liabilities, BTC

var_to_use = small_wealth_lv_bot50[,-which(names(small_wealth_lv_bot50) 
                                          %in% c("Bottom50", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr50 = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
lasso_train50 	= train(Bottom50~Assets+Real.estate+Consumer.durables+
                        Equities+
                        Pension.entitlements+Private.businesses+Other.assets+
                         Liabilities+Home.mortgages+Consumer.credit+
                         Other.liabilities+BTC, 
                     data = small_wealth_lv_bot50[train,],
                           method = "lasso",
                           tuneLength = 25,
                           trControl = trCtr50)

# Predict using the testing data
lasso_pred50 = predict(lasso_train50, newdata = small_wealth_lv_bot50[test,])

# Evaluate performance
lasso_perf50=postResample(pred = lasso_pred50, 
                          obs = small_wealth_lv_bot50[test,"Bottom50"])

lasso_rmse50 = lasso_perf50[1]
```

#### Enet

```{r}
set.seed(3773)
mod_enet50 = cv.glmnet( x = x50, y=y50, alpha = 0.5, nfolds = length(y50))

vip(mod_enet50, num_features = 9, geom = "point")
#Equities, Assets, real estate, consumer durables, home mortgages

#Another plot
plot(mod_enet50$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:13, 
       legend = colnames(small_wealth_lv_bot50[,-1]), cex = .4)

#Optimal lambda:
mod_enet50$lambda.min

variable_importance_enet50=coef(mod_enet50, s='lambda.min')
ordered_var_imp_enet50=variable_importance_enet50[,1][
  order(-abs(variable_importance_enet50[,1]))]
as.data.frame(ordered_var_imp_enet50[ordered_var_imp_enet50 != 0])
imp_vars_enet50=row.names(as.data.frame(ordered_var_imp_enet50[
  ordered_var_imp_enet50 != 0]))
imp_vars_enet50=imp_vars_enet50[-which(imp_vars_enet50 == "(Intercept)")]
cat(imp_vars_enet50, sep="+")

cat(imp_vars_enet50, sep=", ")
#Important variables:
#Equities, Assets, Real.estate, Home.mortgages, Consumer.durables, Other.assets, Other.liabilities, BTC
#^ Same result as lasso 

var_to_use = small_wealth_lv_bot50[,-which(names(small_wealth_lv_bot50) 
                                          %in% c("Bottom50", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr50 = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
enet_train50 	= train(Bottom50~Assets+Real.estate+Consumer.durables+
                        Equities+
                        Pension.entitlements+Private.businesses+Other.assets+
                         Liabilities+Home.mortgages+Consumer.credit+
                         Other.liabilities+BTC, 
                     data = small_wealth_lv_bot50[train,],
                           method = "enet",
                           tuneLength = 25,
                           trControl = trCtr50)

# Predict using the testing data
enet_pred50 = predict(enet_train50, newdata = small_wealth_lv_bot50[test,])

# Evaluate performance
enet_perf50=postResample(pred = enet_pred50, 
                          obs = small_wealth_lv_bot50[test,"Bottom50"])

enet_rmse50 = enet_perf50[1]
```

#### Compare models

```{r}
data.frame("RMSEs"=c(ridge_rmse50, lasso_rmse50, enet_rmse50), 
           row.names = c("Ridge", "Lasso", "Enet") )
```

Will go with lasso because it has smallest rmse

#### Tree
```{r}
set.seed(1)
tree50 = rpart(Bottom50~Assets+Real.estate+Consumer.durables+Equities+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, data = small_wealth_lv_bot50, subset = train)
rpart.plot::rpart.plot(tree50)

tree50_2 = tree(Bottom50~Assets+Real.estate+Consumer.durables+Equities+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, data = small_wealth_lv_bot50, subset = train)

plot(tree50_2)
text(tree50_2, pretty = 0)

```


#### Model

```{r}
mod50 = lm(Bottom50~Equities+Assets+Real.estate+Home.mortgages+Consumer.durables
           +Other.assets+Other.liabilities+BTC,
           data = small_wealth_lv_bot50)
summary(mod50)

#Only equities is stat significant 
# Other assets decrease wealth level? 

par(mfrow=c(2,4))
plot(x = small_wealth_lv_bot50$Assets, y = small_wealth_lv_bot50$Bottom50,
     xlab = "Assets", ylab = "Net Worth", col = "forestgreen")
plot(x = small_wealth_lv_bot50$Equities,y = small_wealth_lv_bot50$Bottom50, col = "cornflowerblue",
     xlab = "Corp Equities", ylab = "Net Worth")
plot(x = small_wealth_lv_bot50$Real.estate,y = small_wealth_lv_bot50$Bottom50, col = "salmon",
     xlab = "Real Estate", ylab = "Net Worth")
plot(x = small_wealth_lv_bot50$Home.mortgages,y = small_wealth_lv_bot50$Bottom50, col = "darkslategray",
     xlab = "Home Mortgages", ylab = "Net Worth")
plot(x = small_wealth_lv_bot50$Consumer.durables,y = small_wealth_lv_bot50$Bottom50, col = "mediumorchid4",
     xlab = "Consumer Durables", ylab = "Net Worth")
plot(x = small_wealth_lv_bot50$Other.assets,y = small_wealth_lv_bot50$Bottom50, col = "gold",
     xlab = "Other Assets", ylab = "Net Worth")
plot(x = small_wealth_lv_bot50$Other.liabilities,y = small_wealth_lv_bot50$Bottom50, col = "brown",
     xlab = "Other Liabilities", ylab = "Net Worth")
plot(x = small_wealth_lv_bot50$BTC,y = small_wealth_lv_bot50$Bottom50, col = "olivedrab",
     xlab = "Bitcoin Return", ylab = "Net Worth")


```

## Next 40%

### Data wrangling

```{r}
set.seed(3773)
head(std_wealth_df)
wealth_lv_next40 = std_wealth_df[std_wealth_df$Category == "Next40",]
head(wealth_lv_next40)

wealth_lv_next40 = wealth_lv_next40 %>% tidyr::spread(Category, Net.worth)

which(wealth_lv_next40$Date== "2015:Q1")
which(wealth_lv_next40$Date== "2021:Q4")

small_wealth_lv_next40 = wealth_lv_next40[103:130, ]
small_wealth_lv_next40$BTC = q_btc_return
head(small_wealth_lv_next40)

x40 =  as.matrix(small_wealth_lv_next40[,-which(names(small_wealth_lv_next40) 
                                            %in% c("Next40", "Date"))])
y40 = small_wealth_lv_next40$Next40
train = sample(1:nrow(x40), nrow(x40)*.75) 
test = (-train)
```

### Select variables

#### Ridge

```{r}
set.seed(3773)
mod_ridge40 = cv.glmnet( x = x40, y=y40, alpha = 0)

vip(mod_ridge40, num_features = 9, geom = "point")
#Equities, Pensions, Assets, other assets, private business, home mortgages

#Another plot
plot(mod_ridge40$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:13, 
       legend = colnames(small_wealth_lv_next40[,-1]), cex = .4)

#Optimal lambda:
mod_ridge40$lambda.min

variable_importance_ridge40=coef(mod_ridge40, s='lambda.min')
ordered_var_imp_ridge40=variable_importance_ridge40[,1][
  order(-abs(variable_importance_ridge40[,1]))]
as.data.frame(ordered_var_imp_ridge40[ordered_var_imp_ridge40 != 0])
imp_vars_ridge40=row.names(as.data.frame(ordered_var_imp_ridge40[
  ordered_var_imp_ridge40 != 0]))
imp_vars_ridge40=imp_vars_ridge40[-which(imp_vars_ridge40 == "(Intercept)")]
cat(imp_vars_ridge40, sep="+")
cat(imp_vars_ridge40, sep=", ")
#Equities, Private.businesses, Pension.entitlements, Other.assets, Assets, 
#Home.mortgages, Real.estate, Liabilities, Consumer.durables, Other.liabilities, Consumer.credit, BTC

var_to_use = small_wealth_lv_next40[,-which(names(small_wealth_lv_next40) 
                                          %in% c("Next40", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr40 = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
ridge_train40 	= train(Next40~Assets+Real.estate+Consumer.durables+Equities+
                        Pension.entitlements+Private.businesses+Other.assets+
                         Liabilities+Home.mortgages+Consumer.credit+
                         Other.liabilities+BTC, 
                     data = small_wealth_lv_next40[train,],
                           method = "ridge",
                           tuneLength = 25,
                           trControl = trCtr40)

# Predict using the testing data
ridge_pred40 = predict(ridge_train40, newdata = small_wealth_lv_next40[test,])

# Evaluate performance
ridge_perf40=postResample(pred = ridge_pred40, 
                          obs = small_wealth_lv_next40[test,"Next40"])

ridge_rmse40 = ridge_perf40[1]
```

#### Lasso

```{r}
set.seed(3773)
mod_lasso40 = cv.glmnet( x = x40, y=y40, alpha = 1)

vip(mod_lasso40, num_features = 9, geom = "point")
#Equities and Assets

#Another plot
plot(mod_lasso40$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:13, 
       legend = colnames(small_wealth_lv_next40[,-1]), cex = .4)

#Optimal lambda:
mod_lasso40$lambda.min

variable_importance_lasso40=coef(mod_lasso40, s='lambda.min')
ordered_var_imp_lasso40=variable_importance_lasso40[,1][
  order(-abs(variable_importance_lasso40[,1]))]
as.data.frame(ordered_var_imp_lasso40[ordered_var_imp_lasso40 != 0])
imp_vars_lasso40=row.names(as.data.frame(ordered_var_imp_lasso40[
  ordered_var_imp_lasso40 != 0]))
imp_vars_lasso40=imp_vars_lasso40[-which(imp_vars_lasso40 == "(Intercept)")]
cat(imp_vars_lasso40, sep="+")
cat(imp_vars_lasso40, sep=", ")
#Assets, Equities, Other.assets, Pension.entitlements

var_to_use = small_wealth_lv_next40[,-which(names(small_wealth_lv_next40) 
                                          %in% c("Next40", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr40 = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
lasso_train40 	= train(Next40~Assets+Real.estate+Consumer.durables+Equities+
                        Pension.entitlements+Private.businesses+Other.assets+
                         Liabilities+Home.mortgages+Consumer.credit+
                         Other.liabilities+BTC, 
                     data = small_wealth_lv_next40[train,],
                           method = "lasso",
                           tuneLength = 25,
                           trControl = trCtr40)

# Predict using the testing data
lasso_pred40 = predict(lasso_train40, newdata = small_wealth_lv_next40[test,])

# Evaluate performance
lasso_perf40=postResample(pred = lasso_pred40, 
                          obs = small_wealth_lv_next40[test,"Next40"])

lasso_rmse40 = lasso_perf40[1]
```

#### Enet

```{r}
set.seed(3773)
mod_enet40 = cv.glmnet( x = x40, y=y40, alpha = 0.5)

vip(mod_enet40, num_features = 9, geom = "point")
#Equities, Pensions, Assets, other assets, private business, 
#home mortgages, real estates


#Another plot
plot(mod_enet40$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:13, 
       legend = colnames(small_wealth_lv_next40[,-1]), cex = .4)

#Optimal lambda:
mod_enet40$lambda.min

variable_importance_enet40=coef(mod_enet40, s='lambda.min')
ordered_var_imp_enet40=variable_importance_enet40[,1][
  order(-abs(variable_importance_enet40[,1]))]
as.data.frame(ordered_var_imp_enet40[ordered_var_imp_enet40 != 0])
imp_vars_enet40=row.names(as.data.frame(ordered_var_imp_enet40[
  ordered_var_imp_enet40 != 0]))
imp_vars_enet40=imp_vars_enet40[-which(imp_vars_enet40 == "(Intercept)")]
cat(imp_vars_enet40, sep="+")
cat(imp_vars_enet40, sep=", ")
#Equities, Pension.entitlements, Private.businesses, Assets, Other.assets, Real.estate, Home.mortgages, Consumer.durables, Other.liabilities

var_to_use = small_wealth_lv_next40[,-which(names(small_wealth_lv_next40) 
                                          %in% c("Next40", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr40 = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
enet_train40 	= train(Next40~Assets+Real.estate+Consumer.durables+Equities+
                        Pension.entitlements+Private.businesses+Other.assets+
                         Liabilities+Home.mortgages+Consumer.credit+
                         Other.liabilities+BTC, 
                     data = small_wealth_lv_next40[train,],
                           method = "enet",
                           tuneLength = 25,
                           trControl = trCtr40)

# Predict using the testing data
enet_pred40 = predict(enet_train40, newdata = small_wealth_lv_next40[test,])

# Evaluate performance
enet_perf40=postResample(pred = enet_pred40, 
                          obs = small_wealth_lv_next40[test,"Next40"])

enet_rmse40 = enet_perf40[1]
```

#### Compare models

```{r}
data.frame("RMSEs"=c(ridge_rmse40, lasso_rmse40, enet_rmse40), 
           row.names = c("Ridge", "Lasso", "Enet") )
```

We will use Lasso's suggested variables

#### Model

```{r}

mod40 = lm(Next40~Assets+Equities+Other.assets+Pension.entitlements,
           data = small_wealth_lv_next40)
summary(mod40)

#How does pension entitlement decrease wealth share

```

## Next 9

```{r}
set.seed(3773)
head(std_wealth_df)
wealth_lv_next9 = std_wealth_df[std_wealth_df$Category == "Next9",]
head(wealth_lv_next9)

wealth_lv_next9 = wealth_lv_next9 %>% tidyr::spread(Category, Net.worth)

which(wealth_lv_next9$Date== "2015:Q1")
which(wealth_lv_next9$Date== "2021:Q4")

small_wealth_lv_next9 = wealth_lv_next9[103:130, ]
small_wealth_lv_next9$BTC = q_btc_return
head(small_wealth_lv_next9)

x9 =  as.matrix(small_wealth_lv_next9[,-which(names(small_wealth_lv_next9) 
                                            %in% c("Next9", "Date"))])
y9 = small_wealth_lv_next9$Next9
train = sample(1:nrow(x9), nrow(x9)*.75) 
test = (-train)
```

### Select variables

#### Ridge

```{r}
set.seed(3773)
mod_ridge9 = cv.glmnet( x = x9, y=y9, alpha = 0)

vip(mod_ridge9, num_features = 9, geom = "point")
#Consumer liabilities, home mortgage, liabilities

?cv.glmnet

#note: it's odd that ridge would choose variables that are colinear

#Another plot
plot(mod_ridge9$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:13, 
       legend = colnames(small_wealth_lv_next9[,-1]), cex = .4)

#Optimal lambda:
mod_ridge9$lambda.min

variable_importance_ridge9=coef(mod_ridge9, s='lambda.min')
ordered_var_imp_ridge9=variable_importance_ridge9[,1][
  order(-abs(variable_importance_ridge9[,1]))]
as.data.frame(ordered_var_imp_ridge9[ordered_var_imp_ridge9 != 0])
imp_vars_ridge9=row.names(as.data.frame(ordered_var_imp_ridge9[
  ordered_var_imp_ridge9 != 0]))
imp_vars_ridge9=imp_vars_ridge9[-which(imp_vars_ridge9 == "(Intercept)")]
cat(imp_vars_ridge9, sep="+")
cat(imp_vars_ridge9, sep=", ")
ordered_var_imp_ridge9
#Consumer.credit, Liabilities, Home.mortgages, Pension.entitlements, Consumer.durables, Private.businesses, Real.estate, Equities, Assets, Other.assets, BTC, Other.liabilities

var_to_use = small_wealth_lv_next9[,-which(names(small_wealth_lv_next9) 
                                          %in% c("Next9", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr9 = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
ridge_train9 	= train(Next9~Assets+Real.estate+Consumer.durables+Equities+
                        Pension.entitlements+Private.businesses+Other.assets+
                         Liabilities+Home.mortgages+Consumer.credit+
                         Other.liabilities+BTC, 
                     data = small_wealth_lv_next9[train,],
                           method = "ridge",
                           tuneLength = 25,
                           trControl = trCtr9)

# Predict using the testing data
ridge_pred9 = predict(ridge_train9, newdata = small_wealth_lv_next9[test,])

# Evaluate performance
ridge_perf9=postResample(pred = ridge_pred9, 
                          obs = small_wealth_lv_next9[test,"Next9"])

ridge_rmse9 = ridge_perf9[1]
```

#### Lasso

```{r}
set.seed(3773)
mod_lasso9 = cv.glmnet( x = x9, y=y9, alpha = 1)

vip(mod_lasso9, num_features = 9, geom = "point")
#=Assets

#Another plot
plot(mod_lasso9$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:13, 
       legend = colnames(small_wealth_lv_next9[,-1]), cex = .4)

#Optimal lambda:
mod_lasso9$lambda.min

variable_importance_lasso9=coef(mod_lasso9, s='lambda.min')
ordered_var_imp_lasso9=variable_importance_lasso9[,1][
  order(-abs(variable_importance_lasso9[,1]))]
as.data.frame(ordered_var_imp_lasso9[ordered_var_imp_lasso9 != 0])
imp_vars_lasso9=row.names(as.data.frame(ordered_var_imp_lasso9[
  ordered_var_imp_lasso9 != 0]))
imp_vars_lasso9=imp_vars_lasso9[-which(imp_vars_lasso9 == "(Intercept)")]
cat(imp_vars_lasso9, sep="+")
cat(imp_vars_lasso9, sep=", ")
#Assets, Pension.entitlements

var_to_use = small_wealth_lv_next9[,-which(names(small_wealth_lv_next9) 
                                          %in% c("Next9", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr9 = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
lasso_train9 	= train(Next9~Assets+Real.estate+Consumer.durables+Equities+
                        Pension.entitlements+Private.businesses+Other.assets+
                         Liabilities+Home.mortgages+Consumer.credit+
                         Other.liabilities+BTC, 
                     data = small_wealth_lv_next9[train,],
                           method = "lasso",
                           tuneLength = 25,
                           trControl = trCtr9)

# Predict using the testing data
lasso_pred9 = predict(lasso_train9, newdata = small_wealth_lv_next9[test,])

# Evaluate performance
lasso_perf9=postResample(pred = lasso_pred9, 
                          obs = small_wealth_lv_next9[test,"Next9"])

lasso_rmse9 = lasso_perf9[1]
```

#### Enet

```{r}
set.seed(3773)
mod_enet9 = cv.glmnet( x = x9, y=y9, alpha = 0.5)

vip(mod_enet9, num_features = 9, geom = "point")
#Pension.entitlements, Equities, Assets, Liabilities, Real.estate, Home.mortgages, Consumer.durables, Other.assets


#Another plot
plot(mod_enet9$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:13, 
       legend = colnames(small_wealth_lv_next9[,-1]), cex = .4)

#Optimal lambda:
mod_enet9$lambda.min

variable_importance_enet9=coef(mod_enet9, s='lambda.min')
ordered_var_imp_enet9=variable_importance_enet9[,1][
  order(-abs(variable_importance_enet9[,1]))]
as.data.frame(ordered_var_imp_enet9[ordered_var_imp_enet9 != 0])
imp_vars_enet9=row.names(as.data.frame(ordered_var_imp_enet9[
  ordered_var_imp_enet9 != 0]))
imp_vars_enet9=imp_vars_enet9[-which(imp_vars_enet9 == "(Intercept)")]
cat(imp_vars_enet9, sep="+")
cat(imp_vars_enet9, sep=", ")
#Pension.entitlements, Equities, Assets, Liabilities, Real.estate, Home.mortgages, Consumer.durables, Other.assets

var_to_use = small_wealth_lv_next9[,-which(names(small_wealth_lv_next9) 
                                          %in% c("Next9", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr9 = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
enet_train9 	= train(Next9~Assets+Real.estate+Consumer.durables+Equities+
                        Pension.entitlements+Private.businesses+Other.assets+
                         Liabilities+Home.mortgages+Consumer.credit+
                         Other.liabilities+BTC, 
                     data = small_wealth_lv_next9[train,],
                           method = "enet",
                           tuneLength = 25,
                           trControl = trCtr9)

# Predict using the testing data
enet_pred9 = predict(enet_train9, newdata = small_wealth_lv_next9[test,])

# Evaluate performance
enet_perf9=postResample(pred = enet_pred9, 
                          obs = small_wealth_lv_next9[test,"Next9"])

enet_rmse9 = enet_perf9[1]
```

#### Compare models

```{r}
data.frame("RMSEs"=c(ridge_rmse9, lasso_rmse9, enet_rmse9), 
           row.names = c("Ridge", "Lasso", "Enet") )
```

Based on rmse, ridge selection would be the best

#### Model

```{r}
mod9 = lm(Next9~Consumer.credit+Consumer.durables+Other.liabilities+Home.mortgages+Liabilities+Private.businesses,
           data = small_wealth_lv_next9)
summary(mod9)

```


# Wealth gap

## Top 1%

```{r}
head(fed_data)
#convert ot %
q_btc_return_perc =100*q_btc_return
#convert from ts to numeric
q_btc_return_perc = as.numeric(q_btc_return_perc)
set.seed(3773)
wealth_sh_top1 = fed_data[fed_data$Category == "Top1",]
head(wealth_sh_top1)

wealth_sh_top1 = wealth_sh_top1 %>% tidyr::spread(Category, Net.worth)

which(wealth_sh_top1$Date== "2015:Q1")
which(wealth_sh_top1$Date== "2021:Q4")

small_wealth_sh_top1 = wealth_sh_top1[103:130, ]


small_wealth_sh_top1$BTC = q_btc_return_perc
head(small_wealth_sh_top1)

x1_share =  as.matrix(small_wealth_sh_top1[,-which(names(small_wealth_sh_top1) 
                                            %in% c("Top1", "Date"))])
y1_share = small_wealth_sh_top1$Top1
train = sample(1:nrow(x1_share), nrow(x1_share)*.75) 
test = (-train)



```





### Ridge
```{r}
set.seed(3773)
mod_ridge_share = cv.glmnet( x = x1_share, y=y1_share, alpha = 0, nfolds = length(y1_share))

vip(mod_ridge_share, num_features = 9, geom = "point")

#Another plot
plot(mod_ridge_share$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:3, 
       legend = colnames(small_wealth_sh_top1[,-1]), cex = .5)

#Optimal lambda:
mod_ridge_share$lambda.min

variable_importance_ridge_share=coef(mod_ridge_share, s='lambda.min')
ordered_var_imp_ridge_share=variable_importance_ridge_share[,1][
  order(-abs(variable_importance_ridge_share[,1]))]
as.data.frame(ordered_var_imp_ridge_share[ordered_var_imp_ridge_share != 0])
imp_vars_ridge_share=row.names(as.data.frame(ordered_var_imp_ridge_share[
  ordered_var_imp_ridge_share != 0]))
imp_vars_ridge_share=imp_vars_ridge_share[-which(imp_vars_ridge_share == "(Intercept)")]
cat(imp_vars_ridge_share, sep="+")
cat(imp_vars_ridge_share, sep=", ")
head(imp_vars_ridge_share)
#Home.mortgages, Consumer.credit, Liabilities, Pension.entitlements, Consumer.durables, Real.estate
var_to_use = small_wealth_sh_top1[,-which(names(small_wealth_sh_top1) 
                                          %in% c("Top1", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
ridge_share_train 	= train(Top1~Assets+Real.estate+Consumer.durables+Corporate.equities.and.mutual.fund.shares+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, 
                     data = small_wealth_sh_top1[train,],
                           method = "ridge",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
ridge_share_pred = predict(ridge_share_train, newdata = small_wealth_sh_top1[test,])

# Evaluate performance
ridge_share_perf=postResample(pred = ridge_share_pred, obs = small_wealth_sh_top1[test,"Top1"])

ridge_share_rmse = ridge_share_perf[1]



```

### Lasso
```{r}
set.seed(3773)
mod_lasso_share = cv.glmnet( x = x1_share, y=y1_share, alpha = 1, nfolds = length(y1_share))

vip(mod_lasso_share, num_features = 9, geom = "point")

#Another plot
plot(mod_lasso_share$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:3, 
       legend = colnames(small_wealth_sh_top1[,-1]), cex = .5)

#Optimal lambda:
mod_lasso_share$lambda.min

variable_importance_lasso_share=coef(mod_lasso_share, s='lambda.min')
ordered_var_imp_lasso_share=variable_importance_lasso_share[,1][
  order(-abs(variable_importance_lasso_share[,1]))]
as.data.frame(ordered_var_imp_lasso_share[ordered_var_imp_lasso_share != 0])
imp_vars_lasso_share=row.names(as.data.frame(ordered_var_imp_lasso_share[
  ordered_var_imp_lasso_share != 0]))
imp_vars_lasso_share=imp_vars_lasso_share[-which(imp_vars_lasso_share == "(Intercept)")]
cat(imp_vars_lasso_share, sep="+")
cat(imp_vars_lasso_share, sep=", ")
head(imp_vars_lasso_share)
#Assets, Liabilities, Consumer.durables, Other.liabilities, BTC
var_to_use = small_wealth_sh_top1[,-which(names(small_wealth_sh_top1) 
                                          %in% c("Top1", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
lasso_share_train 	= train(Top1~Assets+Real.estate+Consumer.durables+Corporate.equities.and.mutual.fund.shares+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, 
                     data = small_wealth_sh_top1[train,],
                           method = "lasso",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
lasso_share_pred = predict(lasso_share_train, newdata = small_wealth_sh_top1[test,])

# Evaluate performance
lasso_share_perf=postResample(pred = lasso_share_pred, obs = small_wealth_sh_top1[test,"Top1"])

lasso_share_rmse = lasso_share_perf[1]



```

### Enet

```{r}
set.seed(3773)
mod_enet_share = cv.glmnet( x = x1_share, y=y1_share, alpha = 0.5, nfolds = length(y1_share))

vip(mod_enet_share, num_features = 9, geom = "point")

#Another plot
plot(mod_enet_share$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:3, 
       legend = colnames(small_wealth_sh_top1[,-1]), cex = .5)

#Optimal lambda:
mod_enet_share$lambda.min

variable_importance_enet_share=coef(mod_enet_share, s='lambda.min')
ordered_var_imp_enet_share=variable_importance_enet_share[,1][
  order(-abs(variable_importance_enet_share[,1]))]
as.data.frame(ordered_var_imp_enet_share[ordered_var_imp_enet_share != 0])
imp_vars_enet_share=row.names(as.data.frame(ordered_var_imp_enet_share[
  ordered_var_imp_enet_share != 0]))
imp_vars_enet_share=imp_vars_enet_share[-which(imp_vars_enet_share == "(Intercept)")]
cat(imp_vars_enet_share, sep="+")
cat(imp_vars_enet_share, sep=", ")
head(imp_vars_enet_share)
#Assets, Liabilities, Consumer.durables, Other.liabilities, BTC
var_to_use = small_wealth_sh_top1[,-which(names(small_wealth_sh_top1) 
                                          %in% c("Top1", "Date"))]
cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
enet_share_train 	= train(Top1~Assets+Real.estate+Consumer.durables+Corporate.equities.and.mutual.fund.shares+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, 
                     data = small_wealth_sh_top1[train,],
                           method = "enet",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
enet_share_pred = predict(enet_share_train, newdata = small_wealth_sh_top1[test,])

# Evaluate performance
enet_share_perf=postResample(pred = enet_share_pred, obs = small_wealth_sh_top1[test,"Top1"])

enet_share_rmse = enet_share_perf[1]


```
#### Comparision

```{r}
sh_df1 = data.frame("CV"=c( ridge_share_rmse, lasso_share_rmse, enet_share_rmse), 
           row.names = c("Ridge", "Lasso", "Enet") )

print(sh_df1)
```
Lasso is the best

### Linear model
```{r}
top1_share_mod = lm(Top1~Assets+Liabilities+Consumer.durables+Other.liabilities+BTC,
                    data = small_wealth_sh_top1)

summary(top1_share_mod)

set.seed(1)
trCtr = trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 3, 
                     search = "random")

lm1_train = train(Top1~Assets+Liabilities+Consumer.durables+Other.liabilities+BTC, 
                 data = small_wealth_sh_top1[train,], 
                  method = "lm", 
                  tuneLength = 20,
                  trControl = trCtr, 
                  preProcess = c("center", "scale"))

lm1_pred = predict(lm1_train, newdata = small_wealth_sh_top1[test,])

lm1_rmse = RMSE(pred= lm1_pred, 
                  small_wealth_sh_top1[test,names(small_wealth_sh_top1)=="Top1"])

```
Bitcoin while decreases wealth share of the top 1%, it is not statistically
significant. We can argue that this is due to cryptoccurrency being a new asset

### Tree

```{r}
set.seed(1)
cat(names(small_wealth_sh_top1)[!names(small_wealth_sh_top1) %in% c("Date", "Top1")], sep = "+")
sh_tree1 = rpart(Top1~Assets+Real.estate+Consumer.durables+
                   Corporate.equities.and.mutual.fund.shares+Pension.entitlements+
                   Private.businesses+Other.assets+Liabilities+Home.mortgages
                 +Consumer.credit+Other.liabilities+BTC, 
                 data = small_wealth_sh_top1, subset = train)

plot(sh_tree1)
rpart.plot(sh_tree1)

set.seed(1)

cp_to_use = sh_tree1$cptable[which.min(sh_tree1$cptable[,"xerror"]) ,"CP"]
nsplit = sh_tree1$cptable[which.min(sh_tree1$cptable[,"xerror"]) ,"nsplit"]

trCtr = trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 3, 
                     search = "random")

tree1_train = train(Top1~Assets+Real.estate+Consumer.durables+
                   Corporate.equities.and.mutual.fund.shares+Pension.entitlements+
                   Private.businesses+Other.assets+Liabilities+Home.mortgages
                 +Consumer.credit+Other.liabilities+BTC, 
                 data = small_wealth_sh_top1[train,], 
                  method = "rpart", 
                  tuneLength = 20,
                  trControl = trCtr, 
                  preProcess = c("center", "scale"),
                  control = rpart.control(minsplit = nsplit, cp = cp_to_use))

tree1_pred = predict(tree1_train, newdata = small_wealth_sh_top1[test,])

tree1_rmse = RMSE(pred= tree1_pred, 
                  small_wealth_sh_top1[test,names(small_wealth_sh_top1)=="Top1"])



```

### Random Forest

```{r}

# ntree_try = seq(200,1000, 50)
# tuneGrid <- expand.grid(.mtry = sqrt(ncol(small_wealth_sh_top1)-2))
# 
# # Find maxnode
# store_maxnode <- list()
# for (maxnodes in c(5: 15)) {
#     set.seed(1234)
#     rf_maxnode <- train(Top1~Assets+Real.estate+Consumer.durables+
#                    Corporate.equities.and.mutual.fund.shares+Pension.entitlements+
#                    Private.businesses+Other.assets+Liabilities+Home.mortgages
#                  +Consumer.credit+Other.liabilities+BTC,
#         data = small_wealth_sh_top1[train,],
#         method = "rf",
#         tuneGrid = tuneGrid,
#         trControl = trCtr,
#         importance = TRUE,
#         maxnodes = maxnodes,
#         ntree = 500)
#     current_iteration <- toString(maxnodes)
#     store_maxnode[[current_iteration]] <- rf_maxnode
# }
# results_mtry <- resamples(store_maxnode)
# summary(results_mtry) #8 nodes is optimal

# store_maxtrees <- list()
# for (ntree in ntree_try) {
#     set.seed(1)
#     rf_maxtrees <- train(Top1~Assets+Real.estate+Consumer.durables+
#                    Corporate.equities.and.mutual.fund.shares+Pension.entitlements+
#                    Private.businesses+Other.assets+Liabilities+Home.mortgages
#                  +Consumer.credit+Other.liabilities+BTC,
#         data = small_wealth_sh_top1[train,],
#         method = "rf",
#         metric = "RMSE",
#         tuneGrid = tuneGrid,
#         trControl = trCtr,
#         importance = TRUE,
#         maxnodes = 8,
#         ntree = ntree)
#     key <- toString(ntree)
#     store_maxtrees[[key]] <- rf_maxtrees
# }
# results_tree <- resamples(store_maxtrees)
# summary(results_tree) #ntree = 500

set.seed(1)
rf1=randomForest(Top1~Assets+Real.estate+Consumer.durables+
                   Corporate.equities.and.mutual.fund.shares+Pension.entitlements+
                   Private.businesses+Other.assets+Liabilities+Home.mortgages
                 +Consumer.credit+Other.liabilities+BTC,
        data = small_wealth_sh_top1[train,], ntree=500, maxnodes=8, 
                importance=TRUE)
rf1_rmse = mean(sqrt(rf1$mse))
varImpPlot(rf1, type=1)

```

RF result: BTC is added to the total while all others add up to 100% -> maybe we need to find anothre way?

#### Model comparison
```{r}
mod1_df= data.frame("CV"=c( rf1_rmse, tree1_rmse, lm1_rmse), 
           row.names = c("Random Forest", "Regression Tree", "Linear") )
print(mod1_df)
```

Linear model has the smallest RMSE

## Bottom 50%
```{r}
set.seed(3773)
wealth_sh_bot50 = fed_data[fed_data$Category == "Bottom50",]
head(wealth_sh_bot50)

wealth_sh_bot50 = wealth_sh_bot50 %>% tidyr::spread(Category, Net.worth)

which(wealth_sh_bot50$Date== "2015:Q1")
which(wealth_sh_bot50$Date== "2021:Q4")

small_wealth_sh_bot50 = wealth_sh_bot50[103:130, ]


small_wealth_sh_bot50$BTC = q_btc_return_perc
head(small_wealth_sh_bot50)

x50_share =  as.matrix(small_wealth_sh_bot50[,-which(names(small_wealth_sh_bot50) 
                                            %in% c("Bottom50", "Date"))])
y50_share = small_wealth_sh_bot50$Bottom50
train = sample(1:nrow(x50_share), nrow(x50_share)*.75) 
test = (-train)

```


### Ridge

```{r}
set.seed(3773)
mod_ridge50_share = cv.glmnet( x = x50_share, y=y50_share, alpha = 0, nfolds = length(y50_share))

vip(mod_ridge50_share, num_features = 9, geom = "point")

#Another plot
plot(mod_ridge50_share$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:3, 
       legend = colnames(small_wealth_sh_top1[,-1]), cex = .5)

#Optimal lambda:
mod_ridge50_share$lambda.min

variable_importance_ridge50_share=coef(mod_ridge50_share, s='lambda.min')
ordered_var_imp_ridge50_share=variable_importance_ridge50_share[,1][
  order(-abs(variable_importance_ridge50_share[,1]))]
as.data.frame(ordered_var_imp_ridge50_share[ordered_var_imp_ridge50_share != 0])
imp_vars_ridge50_share=row.names(as.data.frame(ordered_var_imp_ridge50_share[
  ordered_var_imp_ridge50_share != 0]))
imp_vars_ridge50_share=imp_vars_ridge50_share[-which(imp_vars_ridge50_share == "(Intercept)")]
cat(imp_vars_ridge50_share, sep="+")
cat(imp_vars_ridge50_share, sep=", ")
ordered_var_imp_ridge50_share

# var_to_use = small_wealth_sh_bot50[,-which(names(small_wealth_sh_bot50) 
                                         # %in% c("Bottom50", "Date"))]
# cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
ridge50_share_train 	= train(Bottom50~Assets+Real.estate+Consumer.durables+Corporate.equities.and.mutual.fund.shares+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, 
                     data = small_wealth_sh_bot50[train,],
                           method = "ridge",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
ridge50_share_pred = predict(ridge50_share_train, newdata = small_wealth_sh_bot50[test,])

# Evaluate performance
ridge50_share_perf=postResample(pred = ridge50_share_pred, obs = small_wealth_sh_bot50[test,"Bottom50"])

ridge50_share_rmse = ridge50_share_perf[1]

```

### Lasso
```{r}
set.seed(3773)
mod_lasso50_share = cv.glmnet( x = x50_share, y=y50_share, alpha = 1, nfolds = length(y50_share))

vip(mod_lasso50_share, num_features = 9, geom = "point")

#Another plot
plot(mod_lasso50_share$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:3, 
       legend = colnames(small_wealth_sh_top1[,-1]), cex = .5)



variable_importance_lasso50_share=coef(mod_lasso50_share, s='lambda.min')
ordered_var_imp_lasso50_share=variable_importance_lasso50_share[,1][
  order(-abs(variable_importance_lasso50_share[,1]))]
as.data.frame(ordered_var_imp_lasso50_share[ordered_var_imp_lasso50_share != 0])
imp_vars_lasso50_share=row.names(as.data.frame(ordered_var_imp_lasso50_share[
  ordered_var_imp_lasso50_share != 0]))
imp_vars_lasso50_share=imp_vars_lasso50_share[-which(imp_vars_lasso50_share == "(Intercept)")]
cat(imp_vars_lasso50_share, sep="+")
#Liabilities+Real.estate+Other.assets+Consumer.durables+Corporate.equities.and.mutual.fund.shares+Assets+Consumer.credit+Other.liabilities
ordered_var_imp_lasso50_share

# var_to_use = small_wealth_sh_bot50[,-which(names(small_wealth_sh_bot50) 
                                         # %in% c("Bottom50", "Date"))]
# cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
lasso50_share_train 	= train(Bottom50~Assets+Real.estate+Consumer.durables+Corporate.equities.and.mutual.fund.shares+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, 
                     data = small_wealth_sh_bot50[train,],
                           method = "lasso",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
lasso50_share_pred = predict(lasso50_share_train, newdata = small_wealth_sh_bot50[test,])

# Evaluate performance
lasso50_share_perf=postResample(pred = lasso50_share_pred, obs = small_wealth_sh_bot50[test,"Bottom50"])

lasso50_share_rmse = lasso50_share_perf[1]
```

### Enet

```{r}
set.seed(3773)
mod_enet50_share = cv.glmnet( x = x50_share, y=y50_share, alpha = 0.5, nfolds = length(y50_share))

vip(mod_enet50_share, num_features = 9, geom = "point")

#Another plot
plot(mod_enet50_share$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:3, 
       legend = colnames(small_wealth_sh_top1[,-1]), cex = .5)



variable_importance_enet50_share=coef(mod_enet50_share, s='lambda.min')
ordered_var_imp_enet50_share=variable_importance_enet50_share[,1][
  order(-abs(variable_importance_enet50_share[,1]))]
as.data.frame(ordered_var_imp_enet50_share[ordered_var_imp_enet50_share != 0])
imp_vars_enet50_share=row.names(as.data.frame(ordered_var_imp_enet50_share[
  ordered_var_imp_enet50_share != 0]))
imp_vars_enet50_share=imp_vars_enet50_share[-which(imp_vars_enet50_share == "(Intercept)")]
cat(imp_vars_enet50_share, sep="+")
#Liabilities+Consumer.durables+Real.estate+Other.assets+Assets+Corporate.equities.and.mutual.fund.shares+Pension.entitlements+Home.mortgages+Consumer.credit+Other.liabilities

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
enet50_share_train 	= train(Bottom50~Assets+Real.estate+Consumer.durables+Corporate.equities.and.mutual.fund.shares+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, 
                     data = small_wealth_sh_bot50[train,],
                           method = "enet",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
enet50_share_pred = predict(enet50_share_train, newdata = small_wealth_sh_bot50[test,])

# Evaluate performance
enet50_share_perf=postResample(pred = enet50_share_pred, obs = small_wealth_sh_bot50[test,"Bottom50"])

enet50_share_rmse = enet50_share_perf[1]
```


#### Compare regularization methods
```{r}
sh_df50 = data.frame("CV"=c( ridge50_share_rmse, lasso50_share_rmse, enet50_share_rmse), 
           row.names = c("Ridge", "Lasso", "Enet") )

print(sh_df50)
```

Enet gives the smallest RMSE

### Linear model
```{r}
bot50_share_mod = lm(Bottom50~Liabilities+Consumer.durables+Real.estate+
                       Other.assets+Assets+
                       Corporate.equities.and.mutual.fund.shares+
                       Pension.entitlements+Home.mortgages+Consumer.credit+
                       Other.liabilities,
                    data = small_wealth_sh_bot50)

summary(bot50_share_mod)

set.seed(1)
trCtr = trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 3, 
                     search = "random")

lm50_train = train(Bottom50~Liabilities+Consumer.durables+Real.estate+
                       Other.assets+Assets+
                       Corporate.equities.and.mutual.fund.shares+
                       Pension.entitlements+Home.mortgages+Consumer.credit+
                       Other.liabilities, 
                 data = small_wealth_sh_bot50[train,], 
                  method = "lm", 
                  tuneLength = 20,
                  trControl = trCtr, 
                  preProcess = c("center", "scale"))

lm50_pred = predict(lm50_train, newdata = small_wealth_sh_bot50[test,])

lm50_rmse = RMSE(pred= lm50_pred, 
                  small_wealth_sh_bot50[test,names(small_wealth_sh_bot50)=="Bottom50"])
```

### Tree
```{r}
set.seed(1)

sh_tree50 = rpart(Bottom50~Assets+Real.estate+Consumer.durables+
                   Corporate.equities.and.mutual.fund.shares+Pension.entitlements+
                   Private.businesses+Other.assets+Liabilities+Home.mortgages
                 +Consumer.credit+Other.liabilities+BTC, 
                 data = small_wealth_sh_bot50, subset = train)

plot(sh_tree50)
rpart.plot(sh_tree50)

set.seed(1)

cp_to_use = sh_tree50$cptable[which.min(sh_tree50$cptable[,"xerror"]) ,"CP"]
nsplit = sh_tree50$cptable[which.min(sh_tree50$cptable[,"xerror"]) ,"nsplit"]

trCtr = trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 3, 
                     search = "random")

tree50_train = train(Bottom50~Assets+Real.estate+Consumer.durables+
                   Corporate.equities.and.mutual.fund.shares+Pension.entitlements+
                   Private.businesses+Other.assets+Liabilities+Home.mortgages
                 +Consumer.credit+Other.liabilities+BTC, 
                 data = small_wealth_sh_bot50[train,], 
                  method = "rpart", 
                  tuneLength = 20,
                  trControl = trCtr, 
                  preProcess = c("center", "scale"),
                  control = rpart.control(minsplit = nsplit, cp = cp_to_use))

tree50_pred = predict(tree50_train, newdata = small_wealth_sh_bot50[test,])

tree50_rmse = RMSE(pred= tree50_pred, 
                  small_wealth_sh_bot50[test,names(small_wealth_sh_bot50)=="Bottom50"])
```

### Random forest
```{r}
# ntree_try = seq(200,1000, 50)
# tuneGrid <- expand.grid(.mtry = sqrt(ncol(small_wealth_sh_bot50)-2))
# 
# # Find maxnode
# store_maxnode <- list()
# for (maxnodes in c(5: 15)) {
#     set.seed(1234)
#     rf_maxnode <- train(Bottom50~Assets+Real.estate+Consumer.durables+
#                    Corporate.equities.and.mutual.fund.shares+Pension.entitlements+
#                    Private.businesses+Other.assets+Liabilities+Home.mortgages
#                  +Consumer.credit+Other.liabilities+BTC,
#         data = small_wealth_sh_bot50[train,],
#         method = "rf",
#         tuneGrid = tuneGrid,
#         trControl = trCtr,
#         importance = TRUE,
#         maxnodes = maxnodes,
#         ntree = 500)
#     current_iteration <- toString(maxnodes)
#     store_maxnode[[current_iteration]] <- rf_maxnode
# }
# results_mtry <- resamples(store_maxnode)
# summary(results_mtry) #6 nodes is optimal
# 
# store_maxtrees <- list()
# for (ntree in ntree_try) {
#     set.seed(1)
#     rf_maxtrees <- train(Bottom50~Assets+Real.estate+Consumer.durables+
#                    Corporate.equities.and.mutual.fund.shares+Pension.entitlements+
#                    Private.businesses+Other.assets+Liabilities+Home.mortgages
#                  +Consumer.credit+Other.liabilities+BTC,
#         data = small_wealth_sh_bot50[train,],
#         method = "rf",
#         metric = "RMSE",
#         tuneGrid = tuneGrid,
#         trControl = trCtr,
#         importance = TRUE,
#         maxnodes = 6,
#         ntree = ntree)
#     key <- toString(ntree)
#     store_maxtrees[[key]] <- rf_maxtrees
# }
# results_tree <- resamples(store_maxtrees)
# summary(results_tree) #ntree = 300

set.seed(1)
rf50=randomForest(Bottom50~Assets+Real.estate+Consumer.durables+
                   Corporate.equities.and.mutual.fund.shares+Pension.entitlements+
                   Private.businesses+Other.assets+Liabilities+Home.mortgages
                 +Consumer.credit+Other.liabilities+BTC,
        data = small_wealth_sh_bot50[train,], ntree=300, maxnodes=6, 
                importance=TRUE)
rf50_rmse = mean(sqrt(rf50$mse))
varImpPlot(rf50, type=1)
```

#### Model comparison
```{r}
mod50_df= data.frame("CV"=c( rf50_rmse, tree50_rmse, lm50_rmse), 
           row.names = c("Random Forest", "Regression Tree", "Linear") )
print(mod50_df)
```

Linear model would be the best 

## Next 40%
### Data wrangling
```{r}
set.seed(3773)
wealth_sh_bot40 = fed_data[fed_data$Category == "Next40",]
head(wealth_sh_bot40)

wealth_sh_bot40 = wealth_sh_bot40 %>% tidyr::spread(Category, Net.worth)

which(wealth_sh_bot40$Date== "2015:Q1")
which(wealth_sh_bot40$Date== "2021:Q4")

small_wealth_sh_bot40 = wealth_sh_bot40[103:130, ]


small_wealth_sh_bot40$BTC = q_btc_return_perc
head(small_wealth_sh_bot40)

x40_share =  as.matrix(small_wealth_sh_bot40[,-which(names(small_wealth_sh_bot40) 
                                            %in% c("Next40", "Date"))])
y40_share = small_wealth_sh_bot40$Next40
train = sample(1:nrow(x40_share), nrow(x40_share)*.75) 
test = (-train)
```

### Ridge
```{r}
set.seed(3773)
mod_ridge40_share = cv.glmnet( x = x40_share, y=y40_share, alpha = 0, nfolds = length(y40_share))

vip(mod_ridge40_share, num_features = 9, geom = "point")

#Another plot
plot(mod_ridge40_share$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:3, 
       legend = colnames(small_wealth_sh_top1[,-1]), cex = .5)

#Optimal lambda:
mod_ridge40_share$lambda.min

variable_importance_ridge40_share=coef(mod_ridge40_share, s='lambda.min')
ordered_var_imp_ridge40_share=variable_importance_ridge40_share[,1][
  order(-abs(variable_importance_ridge40_share[,1]))]
as.data.frame(ordered_var_imp_ridge40_share[ordered_var_imp_ridge40_share != 0])
imp_vars_ridge40_share=row.names(as.data.frame(ordered_var_imp_ridge40_share[
  ordered_var_imp_ridge40_share != 0]))
imp_vars_ridge40_share=imp_vars_ridge40_share[-which(imp_vars_ridge40_share == "(Intercept)")]
cat(imp_vars_ridge40_share, sep="+")
cat(imp_vars_ridge40_share, sep=", ")
ordered_var_imp_ridge40_share

# var_to_use = small_wealth_sh_bot40[,-which(names(small_wealth_sh_bot40) 
                                         # %in% c("Next40", "Date"))]
# cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
ridge40_share_train 	= train(Next40~Assets+Real.estate+Consumer.durables+Corporate.equities.and.mutual.fund.shares+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, 
                     data = small_wealth_sh_bot40[train,],
                           method = "ridge",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
ridge40_share_pred = predict(ridge40_share_train, newdata = small_wealth_sh_bot40[test,])

# Evaluate performance
ridge40_share_perf=postResample(pred = ridge40_share_pred, obs = small_wealth_sh_bot40[test,"Next40"])

ridge40_share_rmse = ridge40_share_perf[1]
```

### Lasso
```{r}
set.seed(3773)
mod_lasso40_share = cv.glmnet( x = x40_share, y=y40_share, alpha = 1, nfolds = length(y40_share))

vip(mod_lasso40_share, num_features = 9, geom = "point")

#Another plot
plot(mod_lasso40_share$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:3, 
       legend = colnames(small_wealth_sh_top1[,-1]), cex = .5)

#Optimal lambda:
mod_lasso40_share$lambda.min

variable_importance_lasso40_share=coef(mod_lasso40_share, s='lambda.min')
ordered_var_imp_lasso40_share=variable_importance_lasso40_share[,1][
  order(-abs(variable_importance_lasso40_share[,1]))]
as.data.frame(ordered_var_imp_lasso40_share[ordered_var_imp_lasso40_share != 0])
imp_vars_lasso40_share=row.names(as.data.frame(ordered_var_imp_lasso40_share[
  ordered_var_imp_lasso40_share != 0]))
imp_vars_lasso40_share=imp_vars_lasso40_share[-which(imp_vars_lasso40_share == "(Intercept)")]
cat(imp_vars_lasso40_share, sep="+")
cat(imp_vars_lasso40_share, sep=", ")
ordered_var_imp_lasso40_share

# var_to_use = small_wealth_sh_bot40[,-which(names(small_wealth_sh_bot40) 
                                         # %in% c("Next40", "Date"))]
# cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
lasso40_share_train 	= train(Next40~Assets+Real.estate+Consumer.durables+Corporate.equities.and.mutual.fund.shares+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, 
                     data = small_wealth_sh_bot40[train,],
                           method = "lasso",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
lasso40_share_pred = predict(lasso40_share_train, newdata = small_wealth_sh_bot40[test,])

# Evaluate performance
lasso40_share_perf=postResample(pred = lasso40_share_pred, obs = small_wealth_sh_bot40[test,"Next40"])

lasso40_share_rmse = lasso40_share_perf[1]
```

### Enet
```{r}
set.seed(3773)
mod_enet40_share = cv.glmnet( x = x40_share, y=y40_share, alpha = 0.5, nfolds = length(y40_share))

vip(mod_enet40_share, num_features = 9, geom = "point")

#Another plot
plot(mod_enet40_share$glmnet.fit, "lambda", label=TRUE)
legend("topright", lwd = 1, col = 1:3, 
       legend = colnames(small_wealth_sh_top1[,-1]), cex = .5)

#Optimal lambda:
mod_enet40_share$lambda.min

variable_importance_lasso40_share=coef(mod_lasso40_share, s='lambda.min')
ordered_var_imp_lasso40_share=variable_importance_lasso40_share[,1][
  order(-abs(variable_importance_lasso40_share[,1]))]
as.data.frame(ordered_var_imp_lasso40_share[ordered_var_imp_lasso40_share != 0])
imp_vars_lasso40_share=row.names(as.data.frame(ordered_var_imp_lasso40_share[
  ordered_var_imp_lasso40_share != 0]))
imp_vars_lasso40_share=imp_vars_lasso40_share[-which(imp_vars_lasso40_share == "(Intercept)")]
cat(imp_vars_lasso40_share, sep="+")
cat(imp_vars_lasso40_share, sep=", ")
ordered_var_imp_lasso40_share

# var_to_use = small_wealth_sh_bot40[,-which(names(small_wealth_sh_bot40) 
                                         # %in% c("Next40", "Date"))]
# cat(colnames(var_to_use), sep = "+")

# Cross validation
trCtr = trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 1,
                              search = "random",
                              verboseIter = F)

# Train the model
lasso40_share_train 	= train(Next40~Assets+Real.estate+Consumer.durables+Corporate.equities.and.mutual.fund.shares+Pension.entitlements+Private.businesses+Other.assets+Liabilities+Home.mortgages+Consumer.credit+Other.liabilities+BTC, 
                     data = small_wealth_sh_bot40[train,],
                           method = "lasso",
                           tuneLength = 25,
                           trControl = trCtr)

# Predict using the testing data
lasso40_share_pred = predict(lasso40_share_train, newdata = small_wealth_sh_bot40[test,])

# Evaluate performance
lasso40_share_perf=postResample(pred = lasso40_share_pred, obs = small_wealth_sh_bot40[test,"Next40"])

lasso40_share_rmse = lasso40_share_perf[1]
```

